{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "KoBert 불러오기"
      ],
      "metadata": {
        "id": "Y26MYtGNV9NC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_78dFFu2o0QM",
        "outputId": "bf866f9b-7626-44ed-bce1-21ec20824edf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-tb81ejlr\n",
            "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-tb81ejlr\n",
            "  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting boto3<=1.15.18\n",
            "  Downloading boto3-1.15.18-py2.py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 KB\u001b[0m \u001b[31m704.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gluonnlp<=0.10.0,>=0.6.0\n",
            "  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.5/344.5 KB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mxnet<=1.7.0.post2,>=1.4.0\n",
            "  Downloading mxnet-1.7.0.post2-py2.py3-none-manylinux2014_x86_64.whl (54.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime<=1.8.0,==1.8.0\n",
            "  Downloading onnxruntime-1.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece<=0.1.96,>=0.1.6\n",
            "  Downloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch<=1.10.1,>=1.7.0\n",
            "  Downloading torch-1.10.1-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m880.5/881.9 MB\u001b[0m \u001b[31m134.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 1102397440 bytes == 0x376ce000 @  0x7fefa75de680 0x7fefa75feda2 0x5f714c 0x64d800 0x527022 0x504866 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x5f5ee6 0x56bbe1 0x569d8a 0x5f60c3 0x56cc92 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.9/881.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers<=4.8.1,>=4.8.1\n",
            "  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (3.19.6)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.12)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.21.6)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 KB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.19.0,>=1.18.18\n",
            "  Downloading botocore-1.18.18-py2.py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (0.29.33)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (21.3)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.8/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch<=1.10.1,>=1.7.0->kobert==0.2.3) (4.4.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.9.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (6.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (2022.6.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.26,>=1.20 in /usr/local/lib/python3.8/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (1.24.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (4.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.2.0)\n",
            "Building wheels for collected packages: kobert, gluonnlp, sacremoses\n",
            "  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15708 sha256=edc26f85fb1b17634753c7c0694bbe292e511cca9ab9df728f94d400202c1faa\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dtdnofbj/wheels/bf/5f/74/81bf3a1332130eb6629ecf58876a8746b77021e7d7b0638e91\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp38-cp38-linux_x86_64.whl size=689006 sha256=00360afd1274eefb78a4464c57c5fe0b19dd3276c75ebcf03f238ef9da555246\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/93/9d/2237550c409eb3ed725d6302b7897ddd9a037b40cef66dcd9c\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=30b9151626fc41a573a7e5952b62ce1c347adab34c0fd3d01c5099b864945d6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built kobert gluonnlp sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, torch, sacremoses, onnxruntime, jmespath, graphviz, mxnet, huggingface-hub, gluonnlp, botocore, transformers, s3transfer, boto3, kobert\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 1.10.1 which is incompatible.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.10.1 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed boto3-1.15.18 botocore-1.18.18 gluonnlp-0.10.0 graphviz-0.8.4 huggingface-hub-0.0.12 jmespath-0.10.0 kobert-0.2.3 mxnet-1.7.0.post2 onnxruntime-1.8.0 s3transfer-0.3.7 sacremoses-0.0.53 sentencepiece-0.1.96 tokenizers-0.10.3 torch-1.10.1 transformers-4.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gluonnlp as nlp\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "from transformers import BertModel\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "FcWfpfJNo524"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSOG9VWfo7sv",
        "outputId": "a412f734-668a-4c39-9a7f-a73845e76e6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hC-UotW6o78W",
        "outputId": "6fe0176c-a365-4f3a-e97d-a1632ff65ad8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습했던 모델을 불러오는 과정입니다"
      ],
      "metadata": {
        "id": "q5rS4JDNTVgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from kobert import get_tokenizer\n",
        "from kobert import get_pytorch_kobert_model"
      ],
      "metadata": {
        "id": "TMbha-VLeqp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bertmodel, vocab = get_pytorch_kobert_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl9_RDIlfLct",
        "outputId": "4d214d4c-77b3-4631-c8f6-92db3fcb6184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/.cache/kobert_v1.zip[██████████████████████████████████████████████████]\n",
            "/content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece[██████████████████████████████████████████████████]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jczZOTZ6yeOE",
        "outputId": "f6af483c-61a4-41d6-faab-ce19f3354e46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Vocab(size=8002, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkc0_7TBF47Z",
        "outputId": "b4395568-c3de-4a8f-9117-627bc1d71159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cached model. /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tok.vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t898XJ6SxeN",
        "outputId": "2bcbac11-3462-4294-811c-088f6309ebe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Vocab(size=8002, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1_J1wSyTPnh",
        "outputId": "26a718ff-14e7-4f3b-e4a0-986c57f4d05e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Vocab(size=8002, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "  def __init__(self, bert, hidden_size=768, num_classes=2, dr_rate=None, params=None):\n",
        "    super(BERTClassifier, self).__init__()\n",
        "    self.bert = bert\n",
        "    self.dr_rate = dr_rate\n",
        "\n",
        "    self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "    if dr_rate:\n",
        "      self.dropout = nn.Dropout(p=dr_rate)\n",
        "\n",
        "  def gen_attention_mask(self, token_ids, valid_length):\n",
        "    attention_mask = torch.zeros_like(token_ids)\n",
        "    for i, v in enumerate(valid_length):\n",
        "      attention_mask[i][:v] = 1\n",
        "    return attention_mask.float()\n",
        "\n",
        "  def forward(self, token_ids, valid_length, segment_ids):\n",
        "    attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "    _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "    if self.dr_rate:\n",
        "        out = self.dropout(pooler)\n",
        "    else:\n",
        "        out = pooler\n",
        "    return self.classifier(out)"
      ],
      "metadata": {
        "id": "QKQr799Gr1QC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.state_dict() 로 저장한 모델 불러오기\n",
        "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/이창대_kaggle/11st_shopping.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxnME-gOrRxv",
        "outputId": "ce26c396-2048-4257-b624-2d78cdaf17c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "6AQ5tB-pTw05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "31vwFxbrs6lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 불러온 모델\n",
        "pprint(model, width= 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnuXJ7hJrRsa",
        "outputId": "9c1ad2f4-1b2a-4224-a0e2-26bab15c88c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERTClassifier(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [i[label_idx] for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ],
      "metadata": {
        "id": "SXDYvOzZt7ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(predict_sentence):\n",
        "\n",
        "    data = [predict_sentence, 0]\n",
        "    dataset_another = [data]\n",
        "\n",
        "    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
        "    test_loader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_loader):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "\n",
        "        test_eval=[]\n",
        "        for i in out:\n",
        "            logits=i\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "\n",
        "            if np.argmax(logits) == 0:\n",
        "                test_eval.append(\"부정적\")\n",
        "            else:\n",
        "                test_eval.append(\"긍정적\")\n",
        "\n",
        "        print(\">> 해당 리뷰는 \" + test_eval[0] + \" 리뷰 입니다.\")"
      ],
      "metadata": {
        "id": "MG31dzp3Y6DU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 90 # 3사분위수인 90\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 3\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5"
      ],
      "metadata": {
        "id": "I_vokSbnt_v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    sentence = input(\"긍부정을 판단할 리뷰를 입력해주세요 : \")\n",
        "    if sentence == \"0\":\n",
        "      print(\">> 긍부정 판단을 종료합니다!\")\n",
        "      break\n",
        "    predict(sentence)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bISx_MBdoTr",
        "outputId": "710c7008-78e5-4b9e-ab98-c6c4ce2e2ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "긍부정을 판단할 리뷰를 입력해주세요 : 배송 빠르고 저렴하게 구입했어요 짱짱해서 여름에 자주 빨아 입혀도 깔끔할거 같아요 색깔도 예쁘고 마음에 들어요\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> 해당 리뷰는 긍정적 리뷰 입니다.\n",
            "\n",
            "\n",
            "긍부정을 판단할 리뷰를 입력해주세요 : 손매길이가 짧아요 9부로 입어야할듯요재잴도 별로입니다\n",
            ">> 해당 리뷰는 부정적 리뷰 입니다.\n",
            "\n",
            "\n",
            "긍부정을 판단할 리뷰를 입력해주세요 : 아니 지퍼 꼴이 왜이런거죠...지퍼 수선집에맡겨야겠어요.. 지퍼가 안올라가고 안내려가요 ㅋㅋㅋ 스커트랑 바지 샀는데 지퍼꼬라지 진심... 하..안입은상태에서 지퍼 올렸다내렸다했어요 ㅋㅋㅋ\n",
            ">> 해당 리뷰는 부정적 리뷰 입니다.\n",
            "\n",
            "\n",
            "긍부정을 판단할 리뷰를 입력해주세요 : 0\n",
            ">> 긍부정 판단을 종료합니다!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 긍부정 연관 단어 탐구\n",
        "파인튜닝한 KoBERT 모델에서 단어가 긍/부정에 어떤 영향을 미치는지 알아보려고 합니다. \n",
        "\n",
        "---\n",
        "단어를 임베딩한 가중치(8002*768 행렬)와 마지막 Linear Layer의 가중치(768*2 행렬)과의 행렬곱으로 구하려고 합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "-h5RbcRbT8_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = model.state_dict()"
      ],
      "metadata": {
        "id": "kpGbT3DYtvF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파인 튜닝된 BERT parameter 값\n",
        "# 12 transformer block, 768 hidden size, 12 self-attention heads 로 구성됨\n",
        "list(model.state_dict().keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "885Sq6iavHPj",
        "outputId": "d924e7cb-5392-4e06-cd40-1d2a5a4bc17a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bert.embeddings.position_ids',\n",
              " 'bert.embeddings.word_embeddings.weight',\n",
              " 'bert.embeddings.position_embeddings.weight',\n",
              " 'bert.embeddings.token_type_embeddings.weight',\n",
              " 'bert.embeddings.LayerNorm.weight',\n",
              " 'bert.embeddings.LayerNorm.bias',\n",
              " 'bert.encoder.layer.0.attention.self.query.weight',\n",
              " 'bert.encoder.layer.0.attention.self.query.bias',\n",
              " 'bert.encoder.layer.0.attention.self.key.weight',\n",
              " 'bert.encoder.layer.0.attention.self.key.bias',\n",
              " 'bert.encoder.layer.0.attention.self.value.weight',\n",
              " 'bert.encoder.layer.0.attention.self.value.bias',\n",
              " 'bert.encoder.layer.0.attention.output.dense.weight',\n",
              " 'bert.encoder.layer.0.attention.output.dense.bias',\n",
              " 'bert.encoder.layer.0.attention.output.LayerNorm.weight',\n",
              " 'bert.encoder.layer.0.attention.output.LayerNorm.bias',\n",
              " 'bert.encoder.layer.0.intermediate.dense.weight',\n",
              " 'bert.encoder.layer.0.intermediate.dense.bias',\n",
              " 'bert.encoder.layer.0.output.dense.weight',\n",
              " 'bert.encoder.layer.0.output.dense.bias',\n",
              " 'bert.encoder.layer.0.output.LayerNorm.weight',\n",
              " 'bert.encoder.layer.0.output.LayerNorm.bias',\n",
              " 'bert.encoder.layer.1.attention.self.query.weight',\n",
              " 'bert.encoder.layer.1.attention.self.query.bias',\n",
              " 'bert.encoder.layer.1.attention.self.key.weight',\n",
              " 'bert.encoder.layer.1.attention.self.key.bias',\n",
              " 'bert.encoder.layer.1.attention.self.value.weight',\n",
              " 'bert.encoder.layer.1.attention.self.value.bias',\n",
              " 'bert.encoder.layer.1.attention.output.dense.weight',\n",
              " 'bert.encoder.layer.1.attention.output.dense.bias',\n",
              " 'bert.encoder.layer.1.attention.output.LayerNorm.weight',\n",
              " 'bert.encoder.layer.1.attention.output.LayerNorm.bias',\n",
              " 'bert.encoder.layer.1.intermediate.dense.weight',\n",
              " 'bert.encoder.layer.1.intermediate.dense.bias',\n",
              " 'bert.encoder.layer.1.output.dense.weight',\n",
              " 'bert.encoder.layer.1.output.dense.bias',\n",
              " 'bert.encoder.layer.1.output.LayerNorm.weight',\n",
              " 'bert.encoder.layer.1.output.LayerNorm.bias',\n",
              " 'bert.encoder.layer.2.attention.self.query.weight',\n",
              " 'bert.encoder.layer.2.attention.self.query.bias',\n",
              " 'bert.encoder.layer.2.attention.self.key.weight',\n",
              " 'bert.encoder.layer.2.attention.self.key.bias',\n",
              " 'bert.encoder.layer.2.attention.self.value.weight',\n",
              " 'bert.encoder.layer.2.attention.self.value.bias',\n",
              " 'bert.encoder.layer.2.attention.output.dense.weight',\n",
              " 'bert.encoder.layer.2.attention.output.dense.bias',\n",
              " 'bert.encoder.layer.2.attention.output.LayerNorm.weight',\n",
              " 'bert.encoder.layer.2.attention.output.LayerNorm.bias',\n",
              " 'bert.encoder.layer.2.intermediate.dense.weight',\n",
              " 'bert.encoder.layer.2.intermediate.dense.bias',\n",
              " 'bert.encoder.layer.2.output.dense.weight',\n",
              " 'bert.encoder.layer.2.output.dense.bias',\n",
              " 'bert.encoder.layer.2.output.LayerNorm.weight',\n",
              " 'bert.encoder.layer.2.output.LayerNorm.bias',\n",
              " 'bert.encoder.layer.3.attention.self.query.weight',\n",
              " 'bert.encoder.layer.3.attention.self.query.bias',\n",
              " 'bert.encoder.layer.3.attention.self.key.weight',\n",
              " 'bert.encoder.layer.3.attention.self.key.bias',\n",
              " 'bert.encoder.layer.3.attention.self.value.weight',\n",
              " 'bert.encoder.layer.3.attention.self.value.bias',\n",
              " 'bert.encoder.layer.3.attention.output.dense.weight',\n",
              " 'bert.encoder.layer.3.attention.output.dense.bias',\n",
              " 'bert.encoder.layer.3.attention.output.LayerNorm.weight',\n",
              " 'bert.encoder.layer.3.attention.output.LayerNorm.bias',\n",
              " 'bert.encoder.layer.3.intermediate.dense.weight',\n",
              " 'bert.encoder.layer.3.intermediate.dense.bias',\n",
              " 'bert.encoder.layer.3.output.dense.weight',\n",
              " 'bert.encoder.layer.3.output.dense.bias',\n",
              " 'bert.encoder.layer.3.output.LayerNorm.weight',\n",
              " 'bert.encoder.layer.3.output.LayerNorm.bias',\n",
              " 'bert.encoder.layer.4.attention.self.query.weight',\n",
              " 'bert.encoder.layer.4.attention.self.query.bias',\n",
              " 'bert.encoder.layer.4.attention.self.key.weight',\n",
              " 'bert.encoder.layer.4.attention.self.key.bias',\n",
              " 'bert.encoder.layer.4.attention.self.value.weight',\n",
              " 'bert.encoder.layer.4.attention.self.value.bias',\n",
              " 'bert.encoder.layer.4.attention.output.dense.weight',\n",
              " 'bert.encoder.layer.4.attention.output.dense.bias',\n",
              " 'bert.encoder.layer.4.attention.output.LayerNorm.weight',\n",
              " 'bert.encoder.layer.4.attention.output.LayerNorm.bias',\n",
              " 'bert.encoder.layer.4.intermediate.dense.weight',\n",
              " 'bert.encoder.layer.4.intermediate.dense.bias',\n",
              " 'bert.encoder.layer.4.output.dense.weight',\n",
              " 'bert.encoder.layer.4.output.dense.bias',\n",
              " 'bert.encoder.layer.4.output.LayerNorm.weight',\n",
              " 'bert.encoder.layer.4.output.LayerNorm.bias',\n",
              " 'bert.encoder.layer.5.attention.self.query.weight',\n",
              " 'bert.encoder.layer.5.attention.self.query.bias',\n",
              " 'bert.encoder.layer.5.attention.self.key.weight',\n",
              " 'bert.encoder.layer.5.attention.self.key.bias',\n",
              " 'bert.encoder.layer.5.attention.self.value.weight',\n",
              " 'bert.encoder.layer.5.attention.self.value.bias',\n",
              " 'bert.encoder.layer.5.attention.output.dense.weight',\n",
              " 'bert.encoder.layer.5.attention.output.dense.bias',\n",
              " 'bert.encoder.layer.5.attention.output.LayerNorm.weight',\n",
              " 'bert.encoder.layer.5.attention.output.LayerNorm.bias',\n",
              " 'bert.encoder.layer.5.intermediate.dense.weight',\n",
              " 'bert.encoder.layer.5.intermediate.dense.bias',\n",
              " 'bert.encoder.layer.5.output.dense.weight',\n",
              " 'bert.encoder.layer.5.output.dense.bias',\n",
              " 'bert.encoder.layer.5.output.LayerNorm.weight',\n",
              " 'bert.encoder.layer.5.output.LayerNorm.bias',\n",
              " 'bert.encoder.layer.6.attention.self.query.weight',\n",
              " 'bert.encoder.layer.6.attention.self.query.bias',\n",
              " 'bert.encoder.layer.6.attention.self.key.weight',\n",
              " 'bert.encoder.layer.6.attention.self.key.bias',\n",
              " 'bert.encoder.layer.6.attention.self.value.weight',\n",
              " 'bert.encoder.layer.6.attention.self.value.bias',\n",
              " 'bert.encoder.layer.6.attention.output.dense.weight',\n",
              " 'bert.encoder.layer.6.attention.output.dense.bias',\n",
              " 'bert.encoder.layer.6.attention.output.LayerNorm.weight',\n",
              " 'bert.encoder.layer.6.attention.output.LayerNorm.bias',\n",
              " 'bert.encoder.layer.6.intermediate.dense.weight',\n",
              " 'bert.encoder.layer.6.intermediate.dense.bias',\n",
              " 'bert.encoder.layer.6.output.dense.weight',\n",
              " 'bert.encoder.layer.6.output.dense.bias',\n",
              " 'bert.encoder.layer.6.output.LayerNorm.weight',\n",
              " 'bert.encoder.layer.6.output.LayerNorm.bias',\n",
              " 'bert.encoder.layer.7.attention.self.query.weight',\n",
              " 'bert.encoder.layer.7.attention.self.query.bias',\n",
              " 'bert.encoder.layer.7.attention.self.key.weight',\n",
              " 'bert.encoder.layer.7.attention.self.key.bias',\n",
              " 'bert.encoder.layer.7.attention.self.value.weight',\n",
              " 'bert.encoder.layer.7.attention.self.value.bias',\n",
              " 'bert.encoder.layer.7.attention.output.dense.weight',\n",
              " 'bert.encoder.layer.7.attention.output.dense.bias',\n",
              " 'bert.encoder.layer.7.attention.output.LayerNorm.weight',\n",
              " 'bert.encoder.layer.7.attention.output.LayerNorm.bias',\n",
              " 'bert.encoder.layer.7.intermediate.dense.weight',\n",
              " 'bert.encoder.layer.7.intermediate.dense.bias',\n",
              " 'bert.encoder.layer.7.output.dense.weight',\n",
              " 'bert.encoder.layer.7.output.dense.bias',\n",
              " 'bert.encoder.layer.7.output.LayerNorm.weight',\n",
              " 'bert.encoder.layer.7.output.LayerNorm.bias',\n",
              " 'bert.encoder.layer.8.attention.self.query.weight',\n",
              " 'bert.encoder.layer.8.attention.self.query.bias',\n",
              " 'bert.encoder.layer.8.attention.self.key.weight',\n",
              " 'bert.encoder.layer.8.attention.self.key.bias',\n",
              " 'bert.encoder.layer.8.attention.self.value.weight',\n",
              " 'bert.encoder.layer.8.attention.self.value.bias',\n",
              " 'bert.encoder.layer.8.attention.output.dense.weight',\n",
              " 'bert.encoder.layer.8.attention.output.dense.bias',\n",
              " 'bert.encoder.layer.8.attention.output.LayerNorm.weight',\n",
              " 'bert.encoder.layer.8.attention.output.LayerNorm.bias',\n",
              " 'bert.encoder.layer.8.intermediate.dense.weight',\n",
              " 'bert.encoder.layer.8.intermediate.dense.bias',\n",
              " 'bert.encoder.layer.8.output.dense.weight',\n",
              " 'bert.encoder.layer.8.output.dense.bias',\n",
              " 'bert.encoder.layer.8.output.LayerNorm.weight',\n",
              " 'bert.encoder.layer.8.output.LayerNorm.bias',\n",
              " 'bert.encoder.layer.9.attention.self.query.weight',\n",
              " 'bert.encoder.layer.9.attention.self.query.bias',\n",
              " 'bert.encoder.layer.9.attention.self.key.weight',\n",
              " 'bert.encoder.layer.9.attention.self.key.bias',\n",
              " 'bert.encoder.layer.9.attention.self.value.weight',\n",
              " 'bert.encoder.layer.9.attention.self.value.bias',\n",
              " 'bert.encoder.layer.9.attention.output.dense.weight',\n",
              " 'bert.encoder.layer.9.attention.output.dense.bias',\n",
              " 'bert.encoder.layer.9.attention.output.LayerNorm.weight',\n",
              " 'bert.encoder.layer.9.attention.output.LayerNorm.bias',\n",
              " 'bert.encoder.layer.9.intermediate.dense.weight',\n",
              " 'bert.encoder.layer.9.intermediate.dense.bias',\n",
              " 'bert.encoder.layer.9.output.dense.weight',\n",
              " 'bert.encoder.layer.9.output.dense.bias',\n",
              " 'bert.encoder.layer.9.output.LayerNorm.weight',\n",
              " 'bert.encoder.layer.9.output.LayerNorm.bias',\n",
              " 'bert.encoder.layer.10.attention.self.query.weight',\n",
              " 'bert.encoder.layer.10.attention.self.query.bias',\n",
              " 'bert.encoder.layer.10.attention.self.key.weight',\n",
              " 'bert.encoder.layer.10.attention.self.key.bias',\n",
              " 'bert.encoder.layer.10.attention.self.value.weight',\n",
              " 'bert.encoder.layer.10.attention.self.value.bias',\n",
              " 'bert.encoder.layer.10.attention.output.dense.weight',\n",
              " 'bert.encoder.layer.10.attention.output.dense.bias',\n",
              " 'bert.encoder.layer.10.attention.output.LayerNorm.weight',\n",
              " 'bert.encoder.layer.10.attention.output.LayerNorm.bias',\n",
              " 'bert.encoder.layer.10.intermediate.dense.weight',\n",
              " 'bert.encoder.layer.10.intermediate.dense.bias',\n",
              " 'bert.encoder.layer.10.output.dense.weight',\n",
              " 'bert.encoder.layer.10.output.dense.bias',\n",
              " 'bert.encoder.layer.10.output.LayerNorm.weight',\n",
              " 'bert.encoder.layer.10.output.LayerNorm.bias',\n",
              " 'bert.encoder.layer.11.attention.self.query.weight',\n",
              " 'bert.encoder.layer.11.attention.self.query.bias',\n",
              " 'bert.encoder.layer.11.attention.self.key.weight',\n",
              " 'bert.encoder.layer.11.attention.self.key.bias',\n",
              " 'bert.encoder.layer.11.attention.self.value.weight',\n",
              " 'bert.encoder.layer.11.attention.self.value.bias',\n",
              " 'bert.encoder.layer.11.attention.output.dense.weight',\n",
              " 'bert.encoder.layer.11.attention.output.dense.bias',\n",
              " 'bert.encoder.layer.11.attention.output.LayerNorm.weight',\n",
              " 'bert.encoder.layer.11.attention.output.LayerNorm.bias',\n",
              " 'bert.encoder.layer.11.intermediate.dense.weight',\n",
              " 'bert.encoder.layer.11.intermediate.dense.bias',\n",
              " 'bert.encoder.layer.11.output.dense.weight',\n",
              " 'bert.encoder.layer.11.output.dense.bias',\n",
              " 'bert.encoder.layer.11.output.LayerNorm.weight',\n",
              " 'bert.encoder.layer.11.output.LayerNorm.bias',\n",
              " 'bert.pooler.dense.weight',\n",
              " 'bert.pooler.dense.bias',\n",
              " 'classifier.weight',\n",
              " 'classifier.bias']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(model.state_dict()['classifier.weight'][0]), len(model.state_dict()['classifier.weight'][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-V4v7uWu2LH",
        "outputId": "236c067a-d54d-4be4-cdc6-863ab82d09cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(model.state_dict()['bert.embeddings.word_embeddings.weight'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fbBwyQvwz9B",
        "outputId": "48a124e6-4e57-4bf1-9ec7-cf58fd864fa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8002"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(model.state_dict()['bert.embeddings.word_embeddings.weight'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVEOozai1WPm",
        "outputId": "538807e0-3374-4657-c8d1-98d91f3f16eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = model.state_dict()[\"classifier.weight\"]\n",
        "word_embeddings = model.state_dict()[\"bert.embeddings.word_embeddings.weight\"]"
      ],
      "metadata": {
        "id": "PurdaB6p1WhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_embeddings.shape, weights.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08_QjPkb4XDz",
        "outputId": "535e03ef-7f6b-4918-deee-d86df5996d12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([8002, 768]), torch.Size([2, 768]))"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 부정의 weight\n",
        "weights[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYk5o1h74cqW",
        "outputId": "a3632f50-f627-43c6-c492-bb04c9c50727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([768])"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 긍정의 weight\n",
        "weights[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEgwzHmv5EBL",
        "outputId": "45e33311-717e-4321-c972-7c1a13da8705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([768])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# word_embeddings 에 weights를 transpose 한 값을 곱해(행렬곱) score를 구함\n",
        "word_score = word_embeddings.matmul(weights.T)\n",
        "word_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0IbYuta6wfF",
        "outputId": "82f98e9e-a116-4c5c-b005-1ca182f2680a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0004, -0.0302],\n",
              "        [ 0.0073,  0.0791],\n",
              "        [-0.0043,  0.0120],\n",
              "        ...,\n",
              "        [ 0.0169,  0.0142],\n",
              "        [ 0.0223, -0.0044],\n",
              "        [ 0.0119, -0.0352]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KoBERT에서 사용한 8002개의 단어와 이에 대한 긍/부정 점수를 확인할 수 있다.\n",
        "word_score.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stgMEErbXlE6",
        "outputId": "e3d7a844-cc50-4783-e5d1-5c69e0b6faa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8002, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KoBERT vocab에서 단어 가져오기\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O32-r6D37Zl_",
        "outputId": "3f38c1e1-d1fc-402d-fe33-1b42ae91a80d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Vocab(size=8002, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_list = vocab.idx_to_token\n",
        "print(len(word_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LeaGy_N8adQ",
        "outputId": "396613ef-5f00-4950-e146-0bfb306e1c86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_list, end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq85rMVd8wH8",
        "outputId": "a3e87884-7ac7-4d56-93cd-1c411635fb43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[UNK]', '[PAD]', '[CLS]', '[SEP]', '[MASK]', '!', \"!'\", '!”', '\"', '#', '$', '%', '%)', '&', '&#34;', \"'\", \"'(\", \"',\", '(', '(0', '(1', '(10', '(12', '(15', '(17', '(18', '(19', '(2', '(20', '(23', '(24', '(25', '(3', '(4', '(5', '(6', '(7', '(8', '(9', '(?)', ')', \")'\", '),', ')’', '*', '+', ',', '-', '----------------', '-1', '-2', '-20', '-3', '-4', '.', '...', '...\"', \"...'\", '...”', '/', '0', '0%', '0%)', '0.0', '0.00', '0.1', '0.1%', '0.2%', '0.3', '0.3%', '0.4%', '0.5', '0.5%', '0.6', '0.6%', '0.7', '0.7%', '0.8', '0.8%', '00', '000.0', '00000', '01', '02', '02-', '03', '04', '05', '06', '07', '08', '09', '0:00:00', '1', '1%', '1%)', '1)', '1,000', '1.3%', '1.4%', '1.5%', '1.6', '1.6%', '1.7%', '1.8', '10', '100', '1000', '11', '12', '13', '14', '15', '16', '17', '18', '19', '1⁄2', '1⁄4', '2', '2%', '2)', '2.0', '2.3%', '2.5', '2.5%', '2.8%', '20', '200', '2000', '2011', '2012', '2013', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '3%', '30', '300', '3000', '31', '32', '33', '34', '35', '36', '37', '38', '39', '3⁄4', '4', '4%', '4%)', '40', '400', '4000', '41', '42', '43', '44', '45', '46', '47', '48', '49', '5', '5%', '5%)', '5,000', '50', '500', '5000', '51', '52', '53', '54', '55', '56', '57', '58', '59', '6', '6%', '60', '600', '6000', '61', '62', '63', '64', '65', '66', '67', '68', '69', '7', '7%', '7%)', '7.0', '7.5%', '70', '700', '7000', '71', '72', '73', '74', '75', '76', '77', '78', '79', '8', '8%', '8%)', '80', '800', '8000', '81', '82', '83', '84', '85', '86', '87', '88', '89', '9', '9%', '9%)', '90', '900', '9000', '91', '92', '93', '94', '95', '96', '97', '98', '99', ':', '://', ':00', ';', '<', '=', '=\"\"', '=\"\">', '>', '?', '?\"', '??', '???', '????', '?”', 'A', 'AM', 'AP', 'AR', 'AS', 'AT', 'B', 'BC', 'BO', 'BS', 'C', 'CC', 'CD', 'CI', 'D', 'DB', 'DC', 'DI', 'E', 'ER', 'EU', 'F', 'FC', 'FI', 'FIFA', 'FTA', 'G', 'GB', 'GDP', 'GM', 'H', 'HD', 'I', 'IA', 'IB', 'IC', 'II', 'IM', 'IN', 'IP', 'IS', 'IT', 'J', 'K', 'KB', 'KBS', 'KT', 'L', 'LED', 'LG', 'LPGA', 'LS', 'M', 'MBC', 'MBN', 'MC', 'MOU', 'MS', 'N', 'NA', 'NE', 'NLL', 'NS', 'NTSB', 'New', 'O', 'OC', 'OS', 'OSEN', 'P', 'PC', 'PD', 'PGA', 'PI', 'PM', 'POP', 'PS', 'Q', 'R', 'S', 'SBS', 'SI', 'SK', 'SNS', 'SP', 'SS', 'ST', 'T', 'TI', 'TS', 'TV', 'The', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '^', '_', '`', 'a', 'ab', 'ac', 'ad', 'al', 'all', 'am', 'an', 'ar', 'as', 'at', 'ation', 'ay', 'b', 'bp', 'c', 'ch', 'cm', 'co', 'com', 'ct', 'd', 'e', 'ed', 'el', 'en', 'ent', 'er', 'es', 'est', 'et', 'f', 'g', 'go', 'h', 'ha', 'ho', 'http', 'i', 'ic', 'id', 'il', 'in', 'ing', 'ir', 'is', 'it', 'j', 'k', 'kW', 'kg', 'km', 'kr', 'l', 'le', 'lo', 'm', 'mm', 'n', 'net', 'o', 'ol', 'on', 'or', 'ow', 'p', 'q', 'quot', 'r', 'ra', 're', 'ri', 'ro', 's', 'st', 't', 'ter', 'th', 'tion', 'u', 'ul', 'um', 'un', 'ur', 'us', 'ut', 'v', 'ver', 'w', 'www', 'x', 'y', 'z', '{', '|', '}', '~', '~1', '~20', '~3', '~5', '~6', '~8', '¡', '¤', '§', '\\xad', '®', '°', '±', '¶', '·', '¿', 'Æ', '×', 'Ø', 'ß', 'æ', '÷', 'ø', '́', '̧', 'μ', 'ᄀ', 'ᄋ', 'ᄏ', 'ᄒ', 'ᅵ', 'ᆞ', '―', '‘', '’', '’(', '’,', '“', '”', '”,', '′', '※', '⁄', '↑', '→', '↓', '↓]', '∼', '─', '───', '│', '├', '┼', '▁', '▁\"', '▁&', \"▁'\", \"▁'2013\", '▁(', '▁*', '▁-', '▁/', '▁0', '▁0.2', '▁00:0', '▁1', '▁1%', '▁1,2', '▁1.5', '▁10', '▁10%', '▁100', '▁100%', '▁1000', '▁11', '▁12', '▁120', '▁13', '▁14', '▁15', '▁150', '▁16', '▁17', '▁18', '▁19', '▁1990', '▁1997', '▁1998', '▁1~2', '▁2', '▁20', '▁20%', '▁200', '▁2000', '▁2001', '▁2002', '▁2003', '▁2004', '▁2005', '▁2006', '▁2007', '▁2008', '▁2009', '▁2010', '▁2011', '▁2012', '▁2013', '▁2013-0', '▁2013.0', '▁2013.07.1', '▁2013.07.2', '▁2014', '▁2015', '▁2016', '▁2017', '▁21', '▁22', '▁23', '▁24', '▁25', '▁26', '▁27', '▁28', '▁29', '▁2~3', '▁3', '▁3.0', '▁3.3', '▁30', '▁30%', '▁300', '▁3000', '▁31', '▁32', '▁33', '▁34', '▁35', '▁36', '▁37', '▁38', '▁39', '▁4', '▁40', '▁40%', '▁400', '▁45', '▁48', '▁5', '▁50', '▁50%', '▁500', '▁5000', '▁55', '▁6', '▁60', '▁60%', '▁65', '▁7', '▁70', '▁70%', '▁8', '▁80', '▁80%', '▁9', '▁90', '▁:', '▁<', '▁=', '▁>', '▁?', '▁??', '▁A', '▁APP', '▁B', '▁C', '▁CCTV', '▁CEO', '▁CES', '▁CGV', '▁CJ', '▁D', '▁E', '▁ELS', '▁ETF', '▁F', '▁FA', '▁G', '▁GS', '▁H', '▁HOT', '▁Home', '▁I', '▁IT', '▁J', '▁JTBC', '▁K', '▁KB', '▁KBO', '▁KBS', '▁KDB', '▁KIA', '▁KT', '▁L', '▁LA', '▁LG', '▁LIG', '▁LTE', '▁M', '▁MBC', '▁MC', '▁Mnet', '▁N', '▁NC', '▁NH', '▁NHN', '▁NLL', '▁O', '▁P', '▁PC', '▁PD', '▁Q', '▁QPR', '▁R', '▁S', '▁SBS', '▁SK', '▁SM', '▁SNS', '▁STX', '▁T', '▁TV', '▁U', '▁US', '▁V', '▁VIP', '▁W', '▁XML', '▁YG', '▁[', '▁`', '▁and', '▁c', '▁for', '▁of', '▁the', '▁to', '▁tvN', '▁|', '▁‘', '▁‘2013', '▁“', '▁“‘', '▁│', '▁■', '▁▦', '▁▲', '▁△', '▁▷', '▁◀', '▁◆', '▁◇', '▁【', '▁中', '▁美', '▁가격', '▁가격은', '▁가격이', '▁가계', '▁가계부채', '▁가구', '▁가까운', '▁가까이', '▁가는', '▁가능', '▁가능성', '▁가능성도', '▁가능성을', '▁가능성이', '▁가능하다', '▁가능한', '▁가동', '▁가득', '▁가량', '▁가려', '▁가로', '▁가르', '▁가리', '▁가맹점', '▁가방', '▁가수', '▁가스', '▁가슴', '▁가시', '▁가운데', '▁가입', '▁가입자', '▁가장', '▁가전', '▁가정', '▁가져', '▁가졌다', '▁가족', '▁가족들', '▁가지', '▁가지고', '▁가진', '▁가짜', '▁가치', '▁가치를', '▁각', '▁각각', '▁각자', '▁각종', '▁간', '▁간담회', '▁간부', '▁간사', '▁갈', '▁갈등', '▁갈수록', '▁감', '▁감각', '▁감독', '▁감독님', '▁감독은', '▁감독의', '▁감독이', '▁감동', '▁감면', '▁감사', '▁감사원', '▁감성', '▁감소', '▁감소한', '▁감소했다', '▁감시', '▁감안', '▁감안하면', '▁감염', '▁감정', '▁감추지', '▁감축', '▁갑자기', '▁강', '▁강남', '▁강남구', '▁강력', '▁강력한', '▁강렬한', '▁강릉', '▁강세', '▁강원', '▁강원도', '▁강제', '▁강조', '▁강조했다', '▁강하게', '▁강한', '▁강행', '▁강호동', '▁강화', '▁갖', '▁갖고', '▁갖추', '▁갖춘', '▁갖춰', '▁같', '▁같다', '▁같아', '▁같은', '▁같이', '▁개', '▁개그맨', '▁개념', '▁개막', '▁개발', '▁개방', '▁개별', '▁개봉', '▁개선', '▁개설', '▁개성', '▁개성공단', '▁개인', '▁개인정보', '▁개입', '▁개장', '▁개정', '▁개정안', '▁개척', '▁개최', '▁개최한다', '▁개통', '▁개편', '▁개편안', '▁개혁', '▁객관적', '▁갤럭시', '▁거', '▁거두', '▁거둔', '▁거뒀다', '▁거듭', '▁거래', '▁거래되고', '▁거래량', '▁거론', '▁거리', '▁거부', '▁거의', '▁거절', '▁거주', '▁거짓말', '▁거쳐', '▁거치', '▁거친', '▁걱정', '▁건', '▁건강', '▁건립', '▁건물', '▁건설', '▁건설사', '▁건축', '▁걷', '▁걸', '▁걸그룹', '▁걸린', '▁걸스데이', '▁걸어', '▁걸쳐', '▁검', '▁검거', '▁검사', '▁검색', '▁검증', '▁검찰', '▁검찰에', '▁검찰은', '▁검토', '▁겁니다', '▁것', '▁것과', '▁것도', '▁것에', '▁것으로', '▁것은', '▁것을', '▁것이', '▁것이다', '▁것이라고', '▁것이라는', '▁것이란', '▁것인가', '▁것인지', '▁것입니다', '▁것처럼', '▁게', '▁게다가', '▁게스트', '▁게시물', '▁게시판에', '▁게임', '▁게재', '▁게재했다', '▁겨냥', '▁겨울', '▁격', '▁격려', '▁격차', '▁겪', '▁겪고', '▁견', '▁결', '▁결과', '▁결과를', '▁결국', '▁결론', '▁결별', '▁결승', '▁결심', '▁결정', '▁결정했다', '▁결제', '▁결코', '▁결합', '▁결혼', '▁결혼식', '▁겸', '▁경', '▁경계', '▁경고', '▁경기', '▁경기가', '▁경기도', '▁경기를', '▁경기에서', '▁경기침체', '▁경남', '▁경력', '▁경매', '▁경북', '▁경비', '▁경영', '▁경우', '▁경우가', '▁경쟁', '▁경쟁력', '▁경제', '▁경제성장률', '▁경제적', '▁경찰', '▁경찰관', '▁경찰에', '▁경찰은', '▁경험', '▁계', '▁계기가', '▁계기로', '▁계산', '▁계속', '▁계약', '▁계약을', '▁계열사', '▁계절', '▁계좌', '▁계획', '▁계획을', '▁계획이다', '▁고', '▁고객', '▁고객들', '▁고객에게', '▁고교', '▁고급', '▁고려', '▁고려해', '▁고령', '▁고민', '▁고발', '▁고백', '▁고백했다', '▁고소', '▁고스란히', '▁고양', '▁고영욱', '▁고용', '▁고위', '▁고통', '▁곡', '▁곧', '▁곧바로', '▁골', '▁골든', '▁골키퍼', '▁골프', '▁골프장', '▁곳', '▁곳이', '▁공', '▁공간', '▁공감', '▁공개', '▁공개됐다', '▁공개된', '▁공개했다', '▁공격', '▁공격수', '▁공공', '▁공공기관', '▁공급', '▁공단', '▁공동', '▁공략', '▁공모', '▁공무원', '▁공방', '▁공백', '▁공부', '▁공사', '▁공시', '▁공시했다', '▁공식', '▁공약', '▁공연', '▁공연을', '▁공유', '▁공장', '▁공정', '▁공정위', '▁공포', '▁공항', '▁공화당', '▁과감', '▁과거', '▁과도한', '▁과세', '▁과시', '▁과시했다', '▁과연', '▁과장', '▁과정', '▁과정에서', '▁과정을', '▁과제', '▁과징금', '▁과태료', '▁과학', '▁곽', '▁관', '▁관객', '▁관객들', '▁관계', '▁관계자', '▁관계자는', '▁관계자들', '▁관광', '▁관광객', '▁관람', '▁관련', '▁관련된', '▁관련사진', '▁관련한', '▁관련해', '▁관리', '▁관심', '▁관심을', '▁관심이', '▁관중', '▁관측', '▁관한', '▁관행', '▁광', '▁광고', '▁광주', '▁광주시', '▁괜찮', '▁괴', '▁굉장히', '▁교', '▁교과부', '▁교류', '▁교사', '▁교수', '▁교수는', '▁교육', '▁교육부', '▁교체', '▁교통', '▁교통사고', '▁교환', '▁구', '▁구간', '▁구글', '▁구단', '▁구매', '▁구성', '▁구성된', '▁구속', '▁구속기소', '▁구입', '▁구자철', '▁구조', '▁구조조정', '▁구체적으로', '▁구체적인', '▁구축', '▁구현', '▁국', '▁국가', '▁국가기록원', '▁국가대표', '▁국가안보', '▁국가정보원', '▁국내', '▁국내에서', '▁국내외', '▁국립', '▁국무총리', '▁국무회의', '▁국민', '▁국민들', '▁국민연금', '▁국민은행', '▁국민의', '▁국방', '▁국방부', '▁국세청', '▁국정', '▁국정원', '▁국정조사', '▁국제', '▁국제사회', '▁국조', '▁국채', '▁국토교통부', '▁국토부', '▁국토해양부', '▁국회', '▁국회에서', '▁국회의원', '▁군', '▁군부', '▁군사', '▁굴', '▁궁금', '▁권', '▁권력', '▁권리', '▁권한', '▁귀', '▁귀국', '▁귀여운', '▁귀엽', '▁규명', '▁규모', '▁규모로', '▁규모의', '▁규정', '▁규제', '▁균형', '▁그', '▁그가', '▁그간', '▁그것이', '▁그냥', '▁그녀', '▁그는', '▁그대로', '▁그동안', '▁그때', '▁그래서', '▁그래픽', '▁그랬', '▁그러', '▁그러나', '▁그러면서', '▁그런', '▁그런데', '▁그럼에도', '▁그렇게', '▁그렇지', '▁그려', '▁그려졌다', '▁그룹', '▁그리', '▁그리고', '▁그린', '▁그림', '▁그만큼', '▁그의', '▁그쳤다', '▁극', '▁극대화', '▁극복', '▁극중', '▁극찬', '▁근', '▁근거', '▁근로', '▁근로자', '▁근무', '▁근본', '▁근육', '▁근절', '▁근처', '▁근황', '▁글', '▁글과', '▁글로벌', '▁글을', '▁금', '▁금감원', '▁금리', '▁금메달', '▁금액', '▁금융', '▁금융감독원', '▁금융권', '▁금융기관', '▁금융당국', '▁금융위기', '▁금융투자', '▁금융회사', '▁금지', '▁금품을', '▁급', '▁급격', '▁급등', '▁급락', '▁급여', '▁급증', '▁긍정적', '▁긍정적인', '▁기', '▁기간', '▁기계', '▁기관', '▁기기', '▁기념', '▁기능', '▁기능을', '▁기다리', '▁기대', '▁기대감', '▁기대된다', '▁기대를', '▁기대하고', '▁기대한다', '▁기록', '▁기록하고', '▁기록하며', '▁기록한', '▁기록했다', '▁기반', '▁기반으로', '▁기본', '▁기부', '▁기분', '▁기사', '▁기사본문', '▁기상', '▁기성용', '▁기소', '▁기소된', '▁기술', '▁기아차', '▁기억', '▁기업', '▁기업들', '▁기업의', '▁기여', '▁기온', '▁기울', '▁기재부', '▁기조', '▁기존', '▁기준', '▁기준금리', '▁기준으로', '▁기초', '▁기초자산', '▁기타', '▁기회', '▁기회를', '▁기획', '▁기획재정부', '▁긴', '▁긴급', '▁긴장', '▁긴장감', '▁길', '▁김', '▁김기', '▁김동', '▁김모', '▁김민', '▁김성', '▁김씨', '▁김씨는', '▁김연경', '▁김연아', '▁김영', '▁김용', '▁김용준', '▁김재', '▁김정', '▁김종', '▁김종학', '▁김진', '▁김태', '▁김태희', '▁김한길', '▁김현', '▁깊은', '▁깊이', '▁깔', '▁깜짝', '▁깨', '▁깨끗', '▁꺼', '▁꺾고', '▁꼬리', '▁꼭', '▁꼽았다', '▁꼽히', '▁꼽힌다', '▁꽃', '▁꽃미남', '▁꾸', '▁꾸준한', '▁꾸준히', '▁꿈', '▁꿈꾸', '▁끈다', '▁끊임없', '▁끌고', '▁끌어', '▁끌었다', '▁끝', '▁끝까지', '▁끝나', '▁끝난', '▁끝내', '▁끝에', '▁끼', '▁나', '▁나가', '▁나갈', '▁나누', '▁나눠', '▁나는', '▁나라', '▁나란히', '▁나로호', '▁나머지', '▁나쁜', '▁나서', '▁나선', '▁나선다', '▁나설', '▁나섰', '▁나섰다', '▁나아가', '▁나오', '▁나오고', '▁나오는', '▁나온', '▁나온다', '▁나올', '▁나와', '▁나왔', '▁나왔다', '▁나이', '▁나중에', '▁나타나', '▁나타난', '▁나타났다', '▁나타내', '▁나타냈다', '▁낙', '▁낙찰', '▁난', '▁날', '▁날씨', '▁남', '▁남겨', '▁남겼다', '▁남긴', '▁남녀', '▁남다른', '▁남부', '▁남북', '▁남북관계', '▁남북정상회담', '▁남성', '▁남아', '▁남아있', '▁남은', '▁남자', '▁남자친구', '▁남편', '▁납부', '▁납치', '▁납품', '▁낮', '▁낮아', '▁낮은', '▁낮추', '▁낳', '▁내', '▁내가', '▁내내', '▁내년', '▁내놓', '▁내놨다', '▁내다봤다', '▁내달', '▁내려', '▁내렸다', '▁내리', '▁내린', '▁내부', '▁내수', '▁내야', '▁내용', '▁내용은', '▁내용을', '▁내용의', '▁내용이', '▁낸', '▁냈다', '▁냉', '▁너', '▁너무', '▁넓', '▁넘', '▁넘게', '▁넘겨', '▁넘기', '▁넘는', '▁넘어', '▁넘치는', '▁넣', '▁넣어', '▁네', '▁네이버', '▁네트워크', '▁네티즌', '▁네티즌들', '▁네티즌들은', '▁넥센', '▁노', '▁노동', '▁노동자', '▁노래', '▁노량진', '▁노력', '▁노력을', '▁노력하겠다', '▁노무현', '▁노사', '▁노선', '▁노인', '▁노조', '▁노출', '▁노트북', '▁노하우', '▁노홍철', '▁노후', '▁녹', '▁녹색', '▁녹음', '▁녹화', '▁논', '▁논란', '▁논란이', '▁논리', '▁논의', '▁논쟁', '▁놀', '▁놀라', '▁놀라게', '▁농', '▁농업', '▁농촌', '▁농협', '▁높', '▁높게', '▁높다', '▁높아', '▁높아졌다', '▁높아지고', '▁높았다', '▁높여', '▁높였다', '▁높은', '▁높이', '▁놓', '▁놓고', '▁뇌', '▁뇌물', '▁누', '▁누가', '▁누구', '▁누구나', '▁누군가', '▁누리꾼들', '▁누리꾼들은', '▁누적', '▁누출', '▁눈', '▁눈길을', '▁눈물', '▁눈물을', '▁눈빛', '▁눈에', '▁뉴', '▁뉴스', '▁뉴욕', '▁느껴', '▁느꼈', '▁느끼', '▁느낀', '▁느낄', '▁느낌', '▁늘', '▁늘려', '▁늘리', '▁늘어', '▁늘어나', '▁늘어난', '▁늘어날', '▁늘어났다', '▁늘었다', '▁능력', '▁능력을', '▁늦어', '▁다', '▁다녀', '▁다니', '▁다룬', '▁다르다', '▁다른', '▁다리', '▁다만', '▁다문화', '▁다섯', '▁다소', '▁다수', '▁다시', '▁다양한', '▁다운로드', '▁다음', '▁다음날', '▁다음달', '▁다이어트', '▁다저스', '▁다저스는', '▁다짐', '▁다하겠다', '▁단', '▁단계', '▁단기', '▁단독', '▁단말기', '▁단속', '▁단순', '▁단장', '▁단지', '▁단체', '▁단축', '▁단행', '▁달', '▁달라', '▁달러', '▁달려', '▁달리', '▁달성', '▁달아', '▁달하는', '▁달한다', '▁닮은', '▁담', '▁담겨', '▁담긴', '▁담당', '▁담배', '▁담보', '▁담아', '▁담은', '▁답', '▁답변', '▁답했다', '▁당', '▁당국', '▁당기순이익', '▁당부했다', '▁당분간', '▁당사자', '▁당선', '▁당선인', '▁당시', '▁당연히', '▁당장', '▁당첨', '▁당초', '▁당했다', '▁당황', '▁대', '▁대거', '▁대결', '▁대구', '▁대규모', '▁대기', '▁대기업', '▁대다수', '▁대단', '▁대답', '▁대리점', '▁대만', '▁대법원', '▁대변인', '▁대부분', '▁대북', '▁대비', '▁대사', '▁대상', '▁대상으로', '▁대상자', '▁대선', '▁대신', '▁대안', '▁대외', '▁대우', '▁대응', '▁대전', '▁대중', '▁대중교통', '▁대책', '▁대처', '▁대체', '▁대출', '▁대통령', '▁대통령은', '▁대통령의', '▁대통령이', '▁대통령직', '▁대통령직인수위원회', '▁대폭', '▁대표', '▁대표는', '▁대표단', '▁대표이사', '▁대표적인', '▁대표팀', '▁대학', '▁대학생', '▁대한', '▁대한민국', '▁대한항공', '▁대해', '▁대해서', '▁대해서는', '▁대해서도', '▁대해선', '▁대형', '▁대형마트', '▁대화', '▁대화록', '▁대회', '▁대회에서', '▁댄스', '▁댓글', '▁더', '▁더불어', '▁더욱', '▁덕', '▁덕분에', '▁던지', '▁덜', '▁덧붙였다', '▁데', '▁데뷔', '▁데이터', '▁데이트', '▁도내', '▁도로', '▁도발', '▁도시', '▁도심', '▁도약', '▁도와', '▁도움', '▁도움을', '▁도움이', '▁도입', '▁도전', '▁도중', '▁도착', '▁도쿄', '▁독', '▁독립', '▁독일', '▁독자', '▁독특한', '▁돈', '▁돈을', '▁돌', '▁돌려', '▁돌아', '▁돌아가', '▁돌아오', '▁돌아온', '▁돌입', '▁돌파', '▁돕는', '▁동', '▁동결', '▁동기', '▁동남아', '▁동료', '▁동물', '▁동반', '▁동반성장', '▁동부', '▁동생', '▁동시에', '▁동아시안컵', '▁동아제약', '▁동안', '▁동양', '▁동영상', '▁동원', '▁동의', '▁동작', '▁동참', '▁돼', '▁됐다', '▁되', '▁되고', '▁되는', '▁되면', '▁되어', '▁되지', '▁되찾', '▁된', '▁된다', '▁될', '▁두', '▁두고', '▁두드러', '▁두산', '▁둔', '▁둔화', '▁둘', '▁둘러', '▁둘러싼', '▁둘째', '▁뒤', '▁뒤늦게', '▁뒤집', '▁뒷', '▁뒷받침', '▁드', '▁드라마', '▁드러나', '▁드러난', '▁드러났다', '▁드러내', '▁드러낸', '▁드러냈다', '▁드레스', '▁득점', '▁든다', '▁듣', '▁듣고', '▁들', '▁들고', '▁들려', '▁들어', '▁들어가', '▁들어간', '▁들어갈', '▁들어갔다', '▁들어서', '▁들어오', '▁들었다', '▁들여', '▁듯', '▁듯한', '▁등', '▁등과', '▁등도', '▁등록', '▁등록금', '▁등에', '▁등에서', '▁등으로', '▁등은', '▁등을', '▁등의', '▁등이', '▁등이다', '▁등장', '▁등판', '▁디', '▁디자인', '▁디지털', '▁따', '▁따뜻한', '▁따라', '▁따라서', '▁따로', '▁따르면', '▁따른', '▁딸', '▁땅', '▁땅볼', '▁땅에', '▁때', '▁때까지', '▁때는', '▁때마다', '▁때문', '▁때문에', '▁때문이다', '▁때부터', '▁떠', '▁떠나', '▁떠난', '▁떠오르', '▁떨', '▁떨어져', '▁떨어졌다', '▁떨어지', '▁떨어진', '▁또', '▁또는', '▁또다시', '▁또한', '▁똑같', '▁뚜렷', '▁뛰', '▁뛰어', '▁뛰어난', '▁뛰어넘', '▁뜨거운', '▁뜻', '▁뜻을', '▁띠고', '▁라', '▁라디오', '▁라이벌', '▁라이브', '▁라인업', '▁랭킹', '▁러시아', '▁런던', '▁런던올림픽', '▁레', '▁레드카펫', '▁레알', '▁레이', '▁레전드', '▁로그인', '▁로비', '▁로이킴', '▁로켓', '▁롯데', '▁롯데마트', '▁루', '▁루머', '▁류', '▁류현진', '▁류현진은', '▁리', '▁리그', '▁리더', '▁리더십', '▁리베이트', '▁리스크', '▁리포트', '▁마', '▁마감', '▁마감했다', '▁마드리드', '▁마련', '▁마련했다', '▁마리', '▁마무리', '▁마운드', '▁마을', '▁마음', '▁마음을', '▁마지막', '▁마지막으로', '▁마찬가지', '▁마쳤다', '▁마치', '▁마치고', '▁마친', '▁마케팅', '▁막', '▁막기', '▁막아', '▁막판', '▁만', '▁만기', '▁만나', '▁만난', '▁만날', '▁만남', '▁만났다', '▁만드는', '▁만든', '▁만들', '▁만들고', '▁만들기', '▁만들어', '▁만들었다', '▁만약', '▁만에', '▁만족', '▁만큼', '▁만한', '▁많고', '▁많다', '▁많아', '▁많았', '▁많았다', '▁많은', '▁많이', '▁많지', '▁말', '▁말까지', '▁말레이시아', '▁말씀', '▁말을', '▁말이', '▁말한다', '▁말해', '▁말했다', '▁맛', '▁망', '▁망명', '▁맞', '▁맞는', '▁맞대결', '▁맞서', '▁맞아', '▁맞았다', '▁맞추', '▁맞춘', '▁맞춤형', '▁맞춰', '▁맡', '▁맡고', '▁맡아', '▁맡았', '▁맡았다', '▁맡은', '▁매', '▁매각', '▁매년', '▁매달', '▁매도', '▁매력', '▁매력을', '▁매매', '▁매수', '▁매우', '▁매일', '▁매입', '▁매장', '▁매주', '▁매체', '▁매출', '▁매출액', '▁매출이', '▁맨', '▁맨유', '▁맺', '▁머', '▁머리', '▁머물', '▁먹', '▁먹고', '▁먼저', '▁멀티', '▁멀티미디어', '▁멋진', '▁메', '▁메뉴', '▁메시지', '▁메시지를', '▁메이저', '▁메이저리그', '▁메이크업', '▁메인', '▁멕시코', '▁멘토', '▁멜로', '▁멤버', '▁멤버들', '▁면', '▁면담', '▁면모를', '▁면접', '▁면제', '▁명', '▁명단', '▁명령', '▁명예', '▁명의', '▁명이', '▁명칭', '▁명품', '▁명확', '▁몇', '▁모', '▁모니터링', '▁모델', '▁모두', '▁모든', '▁모르', '▁모르겠다', '▁모른다', '▁모멘텀', '▁모바일', '▁모비스', '▁모색', '▁모습', '▁모습으로', '▁모습을', '▁모습이', '▁모습이다', '▁모아', '▁모았다', '▁모양', '▁모여', '▁모으고', '▁모자', '▁모집', '▁목', '▁목격', '▁목동', '▁목록', '▁목사', '▁목소리', '▁목소리가', '▁목숨', '▁목적', '▁목적으로', '▁목표', '▁목표로', '▁목표주가', '▁몰', '▁몰려', '▁몰아', '▁몸', '▁몸매', '▁못', '▁못하고', '▁못하는', '▁못한', '▁못한다', '▁못할', '▁못해', '▁못했다', '▁묘', '▁무', '▁무게', '▁무기', '▁무대', '▁무대를', '▁무대에', '▁무더위', '▁무려', '▁무료', '▁무료로', '▁무르시', '▁무릎', '▁무리', '▁무산', '▁무상', '▁무슨', '▁무승부', '▁무실점', '▁무엇보다', '▁무역', '▁무이자', '▁무제한', '▁무조건', '▁무죄', '▁묶', '▁문', '▁문서', '▁문의', '▁문자', '▁문재인', '▁문제', '▁문제가', '▁문제는', '▁문제로', '▁문제를', '▁문제에', '▁문제점', '▁문화', '▁문화체육관광부', '▁묻', '▁물', '▁물가', '▁물건', '▁물놀이', '▁물량', '▁물론', '▁물류', '▁물리', '▁물어', '▁물품', '▁뭐', '▁뭔가', '▁뮤지컬', '▁뮤직비디오', '▁미', '▁미국', '▁미국의', '▁미니', '▁미드필더', '▁미디어', '▁미래', '▁미래부', '▁미래창조과학부', '▁미뤄', '▁미리', '▁미만', '▁미모', '▁미사일', '▁미소', '▁미얀마', '▁미치는', '▁미치지', '▁미칠', '▁미투데이', '▁민', '▁민간', '▁민낯', '▁민생', '▁민원', '▁민족', '▁민주', '▁민주당', '▁민주당은', '▁민주주의', '▁민주통합당', '▁믿고', '▁밀', '▁밀려', '▁밀어', '▁및', '▁밑', '▁바', '▁바꾸', '▁바꿔', '▁바뀌', '▁바뀐', '▁바다', '▁바닥', '▁바라', '▁바란다', '▁바람', '▁바로', '▁바이러스', '▁바탕으로', '▁박', '▁박근혜', '▁박명수', '▁박수', '▁박인비', '▁박지성', '▁밖에', '▁밖으로', '▁반', '▁반대', '▁반도체', '▁반드시', '▁반등', '▁반면', '▁반박했다', '▁반발', '▁반복', '▁반영', '▁반응', '▁반응을', '▁반전', '▁받', '▁받게', '▁받고', '▁받기', '▁받는', '▁받는다', '▁받아', '▁받아들여', '▁받아들이', '▁받았', '▁받았다', '▁받으며', '▁받은', '▁받을', '▁받지', '▁발', '▁발견', '▁발견됐다', '▁발굴', '▁발급', '▁발매', '▁발목', '▁발사', '▁발생', '▁발생한', '▁발생할', '▁발생했다', '▁발언', '▁발언을', '▁발전', '▁발탁', '▁발표', '▁발표한', '▁발표했다', '▁발행', '▁발효', '▁발휘', '▁밝은', '▁밝혀', '▁밝혀졌다', '▁밝혔', '▁밝혔다', '▁밝혔습니다', '▁밝히', '▁밝힌', '▁밤', '▁밥', '▁방', '▁방문', '▁방문해', '▁방법', '▁방북', '▁방송', '▁방송되는', '▁방송된', '▁방송된다', '▁방송에서', '▁방식', '▁방식으로', '▁방안', '▁방안을', '▁방지', '▁방침', '▁방침이다', '▁방통위', '▁방향', '▁방향으로', '▁배', '▁배경', '▁배경으로', '▁배당', '▁배려', '▁배우', '▁배우들', '▁배제', '▁배출', '▁배치', '▁배터리', '▁백', '▁백악관', '▁백지영', '▁백화점', '▁버', '▁버냉키', '▁버디', '▁버스', '▁버전', '▁번', '▁번째', '▁벌', '▁벌금', '▁벌써', '▁벌어지', '▁벌어진', '▁벌였다', '▁벌이고', '▁벌이는', '▁벌인', '▁범', '▁범위', '▁범죄', '▁범행', '▁법', '▁법률', '▁법무부', '▁법안', '▁법원', '▁법인', '▁법적', '▁법정', '▁법칙', '▁벗', '▁벗어나', '▁베', '▁베이징', '▁베트남', '▁벤', '▁벤처', '▁벽', '▁변', '▁변경', '▁변동', '▁변동성', '▁변수', '▁변신', '▁변호사', '▁변화', '▁변화를', '▁별', '▁별도로', '▁별도의', '▁병', '▁병역', '▁병원', '▁병행', '▁보', '▁보건', '▁보건복지부', '▁보고', '▁보고서', '▁보관', '▁보급', '▁보기', '▁보내', '▁보낸', '▁보냈다', '▁보는', '▁보니', '▁보다', '▁보도', '▁보도자료', '▁보도했다', '▁보면', '▁보상', '▁보수', '▁보안', '▁보여', '▁보여주', '▁보여준', '▁보여줄', '▁보여줬다', '▁보였다', '▁보완', '▁보유', '▁보유하고', '▁보유한', '▁보육', '▁보이', '▁보이고', '▁보이는', '▁보이며', '▁보이지', '▁보인다', '▁보장', '▁보조금', '▁보증', '▁보통', '▁보험', '▁보험료', '▁보험사', '▁보호', '▁복', '▁복귀', '▁복무', '▁복수', '▁복잡', '▁복지', '▁복합', '▁본', '▁본격', '▁본격적으로', '▁본격적인', '▁본격화', '▁본다', '▁본사', '▁본인', '▁본회의', '▁볼', '▁봉', '▁봉사활동', '▁봐', '▁봤', '▁봤다', '▁부', '▁부각', '▁부과', '▁부담', '▁부담을', '▁부담이', '▁부당', '▁부대', '▁부동산', '▁부르', '▁부모', '▁부모님', '▁부문', '▁부부', '▁부분', '▁부분이', '▁부사장', '▁부산', '▁부산시', '▁부상', '▁부서', '▁부실', '▁부여', '▁부위원장', '▁부인', '▁부작용', '▁부정', '▁부정적', '▁부족', '▁부족한', '▁부지', '▁부진', '▁부채', '▁부처', '▁부총리', '▁부탁', '▁부품', '▁부활', '▁부회장', '▁북', '▁북미', '▁북측', '▁북한', '▁북한의', '▁북한이', '▁분', '▁분노', '▁분당', '▁분들', '▁분류', '▁분리', '▁분명', '▁분명히', '▁분석', '▁분석이다', '▁분석했다', '▁분야', '▁분야에서', '▁분양', '▁분위기', '▁분위기를', '▁분쟁', '▁불', '▁불가능', '▁불가피', '▁불공정', '▁불과', '▁불과하다', '▁불구속', '▁불구하고', '▁불러', '▁불리는', '▁불만을', '▁불법', '▁불안', '▁불투명', '▁불편', '▁불확실성', '▁불황', '▁붕괴', '▁붙', '▁붙잡', '▁브', '▁브라운', '▁브라질', '▁브랜드', '▁브리핑', '▁블랙', '▁블랙박스', '▁블로그', '▁블루', '▁비', '▁비가', '▁비공개', '▁비과세', '▁비교', '▁비교적', '▁비교해', '▁비난', '▁비대위', '▁비대위원장', '▁비롯', '▁비롯한', '▁비롯해', '▁비리', '▁비밀', '▁비상', '▁비서실', '▁비서실장', '▁비스트', '▁비슷', '▁비슷한', '▁비용', '▁비율', '▁비자금', '▁비정규직', '▁비중', '▁비중이', '▁비즈니스', '▁비판', '▁비판했다', '▁비해', '▁비행기', '▁빅', '▁빈', '▁빈소', '▁빌', '▁빌려', '▁빙', '▁빚', '▁빛', '▁빠', '▁빠르게', '▁빠르고', '▁빠른', '▁빠져', '▁빠졌다', '▁빠지', '▁빠진', '▁빨', '▁빨리', '▁빼', '▁빼앗', '▁뽐내', '▁뽐냈다', '▁뽑', '▁뽑아', '▁뿌리', '▁뿐', '▁뿐만', '▁사', '▁사건', '▁사건을', '▁사고', '▁사고가', '▁사고로', '▁사과', '▁사기', '▁사나이', '▁사는', '▁사라지', '▁사람', '▁사람들', '▁사람들이', '▁사람은', '▁사람의', '▁사람이', '▁사랑', '▁사랑을', '▁사례', '▁사로잡', '▁사로잡았다', '▁사망', '▁사망자', '▁사면', '▁사명', '▁사무', '▁사무실', '▁사무총장', '▁사법', '▁사상', '▁사실', '▁사실상', '▁사실을', '▁사실이', '▁사안', '▁사업', '▁사업을', '▁사업자', '▁사연', '▁사용', '▁사용자', '▁사용하는', '▁사용할', '▁사유', '▁사이', '▁사이버', '▁사이에', '▁사이에서', '▁사이트', '▁사장', '▁사장은', '▁사전', '▁사정', '▁사진', '▁사진을', '▁사진이', '▁사태', '▁사퇴', '▁사항', '▁사회', '▁사회공헌', '▁사회복지', '▁사회적', '▁사흘', '▁삭감', '▁삭제', '▁산', '▁산업', '▁산하', '▁살', '▁살아', '▁살인', '▁살펴', '▁살펴보면', '▁살해', '▁삶', '▁삼', '▁삼성', '▁삼성생명', '▁삼성전자', '▁삼성전자는', '▁삼성화재', '▁삼진', '▁삼청동', '▁상', '▁상담', '▁상당', '▁상당수', '▁상당의', '▁상당한', '▁상대', '▁상대로', '▁상대적으로', '▁상무', '▁상반기', '▁상생', '▁상승', '▁상승세', '▁상승세를', '▁상승한', '▁상승했다', '▁상위', '▁상임위', '▁상장', '▁상징', '▁상처', '▁상태', '▁상태다', '▁상태에서', '▁상품', '▁상하이', '▁상한가', '▁상향', '▁상호', '▁상환', '▁상황', '▁상황에', '▁상황에서', '▁상황을', '▁상황이', '▁상황이다', '▁새', '▁새누리당', '▁새누리당은', '▁새로', '▁새로운', '▁새롭게', '▁새벽', '▁새해', '▁샌프란시스코', '▁생', '▁생각', '▁생각을', '▁생각이', '▁생각한다', '▁생각해', '▁생기', '▁생긴', '▁생명', '▁생방송', '▁생산', '▁생존', '▁생태계', '▁생활', '▁서', '▁서귀포', '▁서로', '▁서류', '▁서민', '▁서부', '▁서비스', '▁서비스를', '▁서울', '▁서울대', '▁서울시', '▁서울중앙지검', '▁서울중앙지법', '▁서초구', '▁석', '▁석유', '▁선', '▁선거', '▁선고', '▁선고받', '▁선고했다', '▁선두', '▁선물', '▁선박', '▁선발', '▁선발투수', '▁선배', '▁선보여', '▁선보였다', '▁선보이며', '▁선보인', '▁선보인다', '▁선보일', '▁선사', '▁선생님', '▁선수', '▁선수가', '▁선수단', '▁선수들', '▁선수들이', '▁선언', '▁선예', '▁선임', '▁선정', '▁선정됐다', '▁선정된', '▁선제골', '▁선진국', '▁선출', '▁선택', '▁선호', '▁설', '▁설계', '▁설득', '▁설립', '▁설명', '▁설명이다', '▁설명했다', '▁설문조사', '▁설비', '▁설정', '▁설치', '▁섬', '▁성', '▁성격', '▁성공', '▁성공했다', '▁성과', '▁성과를', '▁성남', '▁성능', '▁성동일', '▁성매매', '▁성명을', '▁성장', '▁성장률', '▁성장세', '▁성적', '▁성추행', '▁성폭력', '▁성폭행', '▁성향', '▁성형', '▁세', '▁세계', '▁세계적인', '▁세금', '▁세대', '▁세력', '▁세무조사', '▁세미나', '▁세부', '▁세븐', '▁세상', '▁세상을', '▁세우', '▁세워', '▁세웠다', '▁세종', '▁세종시', '▁세트', '▁섹시', '▁셀카', '▁셈이다', '▁소', '▁소감을', '▁소개', '▁소개했다', '▁소녀', '▁소녀시대', '▁소득', '▁소리', '▁소방', '▁소방당국', '▁소비', '▁소비자', '▁소비자들', '▁소설', '▁소셜', '▁소속', '▁소속사', '▁소송', '▁소식에', '▁소식을', '▁소식통', '▁소요', '▁소유', '▁소장', '▁소재', '▁소중한', '▁소지섭', '▁소집', '▁소통', '▁소폭', '▁소프트웨어', '▁소형', '▁소화', '▁소환', '▁속', '▁속도', '▁속에', '▁속에서', '▁손', '▁손님', '▁손실', '▁손연재', '▁손을', '▁손해배상', '▁손흥민', '▁솔로', '▁솔루션', '▁송', '▁쇼', '▁쇼핑', '▁수', '▁수급', '▁수도', '▁수도권', '▁수립', '▁수많은', '▁수목드라마', '▁수밖에', '▁수비', '▁수비수', '▁수사', '▁수사를', '▁수상', '▁수석', '▁수석대표', '▁수수료', '▁수술', '▁수십', '▁수업', '▁수영', '▁수요', '▁수요가', '▁수용', '▁수원', '▁수익', '▁수익률', '▁수익성', '▁수익을', '▁수입', '▁수정', '▁수주', '▁수준', '▁수준으로', '▁수준이다', '▁수출', '▁수치', '▁수치다', '▁수행', '▁수혜', '▁숙', '▁순', '▁순간', '▁순매도', '▁순매수', '▁순위', '▁순이익', '▁술', '▁숨', '▁숨졌다', '▁숨진', '▁숫자', '▁쉬', '▁쉽게', '▁쉽지', '▁슈', '▁슈팅', '▁슈퍼', '▁스', '▁스노든', '▁스마트', '▁스마트폰', '▁스몰캡', '▁스스로', '▁스위스', '▁스케줄', '▁스크린', '▁스타', '▁스타일', '▁스태프', '▁스토리', '▁스트레스', '▁스페셜올림픽', '▁스페인', '▁스포츠', '▁스포츠조선닷컴', '▁스피드', '▁슬', '▁승', '▁승객', '▁승리', '▁승리를', '▁승무원', '▁승부', '▁승부주', '▁승용차', '▁승인', '▁승진', '▁시', '▁시가총액', '▁시각', '▁시간', '▁시간을', '▁시간이', '▁시기', '▁시나리오', '▁시너지', '▁시는', '▁시달리', '▁시대', '▁시도', '▁시리아', '▁시리즈', '▁시민', '▁시민단체', '▁시민들', '▁시범', '▁시사', '▁시상식', '▁시선을', '▁시설', '▁시스템', '▁시스템을', '▁시신', '▁시위', '▁시작', '▁시작된', '▁시작으로', '▁시작한', '▁시작했다', '▁시장', '▁시장에', '▁시장에서', '▁시장은', '▁시장의', '▁시절', '▁시점', '▁시즌', '▁시청률', '▁시청자', '▁시청자들', '▁시청자들의', '▁시카고', '▁시행', '▁시험', '▁식', '▁식당', '▁식사', '▁식품', '▁신', '▁신경', '▁신고', '▁신곡', '▁신규', '▁신동엽', '▁신뢰', '▁신문', '▁신분', '▁신설', '▁신세계', '▁신속', '▁신시내티', '▁신용', '▁신용등급', '▁신용카드', '▁신인', '▁신임', '▁신제품', '▁신중', '▁신청', '▁신한금융투자', '▁신한은행', '▁신호', '▁신화', '▁신흥국', '▁실', '▁실내', '▁실력', '▁실망', '▁실무', '▁실무회담', '▁실수', '▁실시', '▁실시간', '▁실시한', '▁실시한다', '▁실적', '▁실적이', '▁실제', '▁실제로', '▁실종', '▁실질적인', '▁실천', '▁실태', '▁실패', '▁실행', '▁실험', '▁실현', '▁싫어', '▁심', '▁심각', '▁심각한', '▁심경', '▁심리', '▁심사', '▁심사위원', '▁심지어', '▁심판', '▁심화', '▁싱가포르', '▁싱글', '▁싶다', '▁싶어', '▁싶었', '▁싶은', '▁싸', '▁싸이', '▁쌍용차', '▁쌓', '▁써', '▁써니', '▁썼다', '▁쏟아', '▁쓰', '▁쓰레기', '▁쓴', '▁쓸', '▁씨', '▁씨가', '▁씨는', '▁씨스타', '▁씨엔블루', '▁아', '▁아끼', '▁아나운서', '▁아내', '▁아니', '▁아니냐', '▁아니냐는', '▁아니다', '▁아니라', '▁아니면', '▁아니었다', '▁아니지만', '▁아닌', '▁아동', '▁아들', '▁아래', '▁아름', '▁아름다운', '▁아무', '▁아버지', '▁아베', '▁아빠', '▁아쉬움', '▁아시아', '▁아시아나', '▁아시아나항공', '▁아예', '▁아울러', '▁아이', '▁아이돌', '▁아이들', '▁아이디어', '▁아이유', '▁아이템', '▁아이폰', '▁아주', '▁아직', '▁아침', '▁아파트', '▁아픔', '▁악', '▁악화', '▁안', '▁안내', '▁안보', '▁안에', '▁안전', '▁안정', '▁안정적인', '▁안타', '▁안타를', '▁안팎', '▁앉아', '▁않', '▁않게', '▁않겠다', '▁않고', '▁않기', '▁않는', '▁않는다', '▁않다', '▁않도록', '▁않아', '▁않았', '▁않았다', '▁않았던', '▁않았지만', '▁않으', '▁않으면', '▁않은', '▁않을', '▁않을까', '▁않지만', '▁알', '▁알게', '▁알고', '▁알려', '▁알려져', '▁알려졌다', '▁알려진', '▁알렸다', '▁알리', '▁알아', '▁알제리', '▁암', '▁압', '▁압도적', '▁압력', '▁압류', '▁압박', '▁압수수색', '▁앞', '▁앞두고', '▁앞둔', '▁앞서', '▁앞선', '▁앞세워', '▁앞에', '▁앞에서', '▁앞으로', '▁앞장서', '▁애', '▁애널리스트', '▁애니메이션', '▁애리조나', '▁애정', '▁애초', '▁애플', '▁애플리케이션', '▁액션', '▁앤', '▁앨범', '▁앱', '▁야', '▁야간', '▁야구', '▁야당', '▁야외', '▁약', '▁약세', '▁약속', '▁양', '▁양국', '▁양성', '▁양적완화', '▁양측', '▁얘기', '▁어', '▁어깨', '▁어느', '▁어디', '▁어떤', '▁어떻게', '▁어려운', '▁어려울', '▁어려움', '▁어려움을', '▁어려워', '▁어렵', '▁어렵다', '▁어린', '▁어린이', '▁어린이집', '▁어머니', '▁어울리', '▁어제', '▁억울', '▁억제', '▁언', '▁언급', '▁언급했다', '▁언론', '▁언제', '▁얻고', '▁얻어', '▁얻었다', '▁얻은', '▁얻을', '▁얼굴', '▁얼마', '▁얼마나', '▁얼음', '▁엄', '▁엄격', '▁엄마', '▁엄청난', '▁엄태웅', '▁업', '▁업계', '▁업그레이드', '▁업데이트', '▁업무', '▁업무를', '▁업무보고', '▁업종', '▁업체', '▁업체들', '▁없', '▁없고', '▁없는', '▁없다', '▁없다고', '▁없다는', '▁없도록', '▁없애', '▁없어', '▁없었', '▁없었다', '▁없었던', '▁없을', '▁없이', '▁없지만', '▁에너지', '▁에릭', '▁에스', '▁에어컨', '▁에이', '▁에이스', '▁에피소드', '▁엔', '▁엔진', '▁엔화', '▁엘', '▁엠', '▁엠넷', '▁여', '▁여객기', '▁여건', '▁여기', '▁여기에', '▁여당', '▁여러', '▁여러분', '▁여론', '▁여름', '▁여름철', '▁여배우', '▁여부', '▁여부를', '▁여성', '▁여성들', '▁여신', '▁여야', '▁여유', '▁여의도', '▁여자', '▁여자친구', '▁여전히', '▁여행', '▁역', '▁역대', '▁역량', '▁역사', '▁역사적', '▁역시', '▁역을', '▁역전', '▁역할', '▁역할을', '▁연', '▁연간', '▁연결', '▁연계', '▁연구', '▁연구개발', '▁연구원은', '▁연극', '▁연금', '▁연기', '▁연기를', '▁연락', '▁연령', '▁연말', '▁연방', '▁연봉', '▁연세대', '▁연속', '▁연습', '▁연애', '▁연예', '▁연예병사', '▁연예인', '▁연인', '▁연장', '▁연출', '▁열', '▁열고', '▁열람', '▁열렸다', '▁열리', '▁열리는', '▁열린', '▁열린다', '▁열릴', '▁열심히', '▁열애', '▁열애설', '▁열어', '▁열었다', '▁열정', '▁열차', '▁염', '▁염두에', '▁영', '▁영광', '▁영국', '▁영등포', '▁영상', '▁영어', '▁영업', '▁영업이익', '▁영업이익은', '▁영업익', '▁영업정지', '▁영역', '▁영입', '▁영하', '▁영향', '▁영향력', '▁영향으로', '▁영향을', '▁영화', '▁옆', '▁예', '▁예고', '▁예금', '▁예능', '▁예능프로그램', '▁예방', '▁예비', '▁예쁘', '▁예산', '▁예상', '▁예상되는', '▁예상된다', '▁예상치', '▁예상했다', '▁예술', '▁예약', '▁예전', '▁예정', '▁예정이다', '▁예측', '▁옛', '▁오', '▁오는', '▁오늘', '▁오늘의', '▁오디션', '▁오래', '▁오랜', '▁오랫동안', '▁오르', '▁오른', '▁오름세', '▁오바마', '▁오빠', '▁오연서', '▁오전', '▁오픈', '▁오피스텔', '▁오후', '▁오히려', '▁옥', '▁온', '▁온라인', '▁올', '▁올라', '▁올랐', '▁올랐다', '▁올려', '▁올렸다', '▁올리', '▁올린', '▁올림픽', '▁올스타', '▁올스타전', '▁올해', '▁올해부터', '▁옮겨', '▁옮기', '▁옷', '▁완', '▁완공', '▁완료', '▁완벽', '▁완벽한', '▁완성', '▁완전', '▁완전히', '▁완화', '▁왔다', '▁왕', '▁왜', '▁왜곡', '▁외', '▁외교', '▁외교부', '▁외국', '▁외국인', '▁외모', '▁외부', '▁외에', '▁외에도', '▁외환', '▁외환은행', '▁왼쪽', '▁요', '▁요구', '▁요구하는', '▁요구했다', '▁요금', '▁요금제', '▁요리', '▁요소', '▁요인', '▁요즘', '▁요청', '▁요청했다', '▁욕', '▁욕심', '▁용', '▁용산', '▁용의자', '▁용인', '▁우', '▁우려', '▁우려가', '▁우리', '▁우리가', '▁우리나라', '▁우리는', '▁우리은행', '▁우리투자증권', '▁우선', '▁우수', '▁우승', '▁우승을', '▁우주', '▁우즈', '▁운', '▁운동', '▁운명', '▁운송', '▁운영', '▁운영하는', '▁운용', '▁운전', '▁운전자', '▁운항', '▁운행', '▁울', '▁울산', '▁움직이', '▁움직임', '▁웃', '▁웃음을', '▁워', '▁워낙', '▁워싱턴', '▁원', '▁원내대변인', '▁원내대표', '▁원내대표는', '▁원래', '▁원인', '▁원장', '▁원전', '▁원정', '▁원정경기', '▁원칙', '▁원하는', '▁원화', '▁원활', '▁월드', '▁월드컵', '▁월화드라마', '▁웨딩', '▁웹', '▁위', '▁위기', '▁위로', '▁위반', '▁위안부', '▁위원', '▁위원장', '▁위원장은', '▁위조', '▁위촉', '▁위축', '▁위치', '▁위치한', '▁위탁', '▁위한', '▁위해', '▁위해서', '▁위해서는', '▁위헌', '▁위험', '▁위협', '▁윌리엄', '▁유', '▁유가증권시장', '▁유감', '▁유나이티드', '▁유니폼', '▁유도', '▁유동성', '▁유럽', '▁유력', '▁유로존', '▁유리', '▁유명', '▁유사', '▁유엔', '▁유일', '▁유입', '▁유재석', '▁유족', '▁유지', '▁유출', '▁유치', '▁유통', '▁유튜브', '▁육', '▁육군', '▁육박', '▁육성', '▁윤', '▁윤씨', '▁윤창중', '▁은퇴', '▁은행', '▁음', '▁음반', '▁음성', '▁음식', '▁음악', '▁음원', '▁응', '▁응급', '▁응답', '▁응답자', '▁응시', '▁응원', '▁의견', '▁의견을', '▁의견이', '▁의결', '▁의도', '▁의뢰', '▁의료', '▁의류', '▁의무', '▁의문', '▁의미', '▁의사', '▁의상', '▁의식', '▁의심', '▁의약품', '▁의원', '▁의원들', '▁의원은', '▁의원이', '▁의장', '▁의존', '▁의지', '▁의지를', '▁의한', '▁의해', '▁의혹', '▁의혹을', '▁의회', '▁이', '▁이같은', '▁이같이', '▁이것', '▁이곳', '▁이끄는', '▁이끌', '▁이끌어', '▁이끌었다', '▁이날', '▁이내', '▁이는', '▁이달', '▁이대호', '▁이데일리', '▁이동', '▁이동통신', '▁이동흡', '▁이들', '▁이들은', '▁이들의', '▁이들이', '▁이라크', '▁이래', '▁이러한', '▁이런', '▁이렇게', '▁이례적', '▁이로써', '▁이루', '▁이뤄', '▁이뤄졌다', '▁이뤄지', '▁이뤄진', '▁이뤄질', '▁이르는', '▁이르면', '▁이른다', '▁이른바', '▁이를', '▁이름', '▁이름을', '▁이마트', '▁이메일', '▁이명박', '▁이미', '▁이미지', '▁이미지를', '▁이민', '▁이밖에', '▁이번', '▁이번에', '▁이벤트', '▁이벤트를', '▁이병헌', '▁이사장', '▁이사회', '▁이상', '▁이상의', '▁이서진', '▁이수', '▁이슈', '▁이스라엘', '▁이슬람', '▁이승', '▁이승엽', '▁이시영', '▁이야기', '▁이야기를', '▁이어', '▁이어가고', '▁이어갔다', '▁이어졌다', '▁이어지고', '▁이어지는', '▁이어진', '▁이어질', '▁이에', '▁이와', '▁이용', '▁이용자', '▁이용한', '▁이용할', '▁이용해', '▁이웃', '▁이유', '▁이유는', '▁이유로', '▁이유를', '▁이익', '▁이재', '▁이적', '▁이전', '▁이정', '▁이정현', '▁이제', '▁이종', '▁이종석', '▁이준', '▁이중', '▁이집트', '▁이처럼', '▁이탈리아', '▁이틀', '▁이하', '▁이해', '▁이행', '▁이혼', '▁이효리', '▁이후', '▁익', '▁인', '▁인간', '▁인구', '▁인권', '▁인근', '▁인기', '▁인기를', '▁인도', '▁인도네시아', '▁인력', '▁인물', '▁인사', '▁인사들', '▁인사를', '▁인사청문회', '▁인상', '▁인생', '▁인선', '▁인쇄', '▁인수', '▁인수위', '▁인수위원', '▁인수위원회', '▁인식', '▁인연', '▁인원', '▁인재', '▁인정', '▁인정받', '▁인증', '▁인증샷', '▁인질', '▁인천', '▁인천공항', '▁인천시', '▁인터', '▁인터넷', '▁인터뷰', '▁인터뷰에서', '▁인턴기자', '▁인프라', '▁인피니트', '▁인하', '▁인한', '▁인해', '▁일', '▁일각에서는', '▁일단', '▁일대', '▁일반', '▁일방적', '▁일본', '▁일본의', '▁일부', '▁일상', '▁일어나', '▁일어난', '▁일으키', '▁일으킨', '▁일을', '▁일이', '▁일자리', '▁일정', '▁일제히', '▁일주일', '▁일환으로', '▁읽', '▁잃은', '▁임', '▁임금', '▁임기', '▁임대', '▁임명', '▁임시', '▁임시국회', '▁임신', '▁임원', '▁임직원', '▁입', '▁입건', '▁입고', '▁입단', '▁입력', '▁입법', '▁입원', '▁입은', '▁입장', '▁입장을', '▁입장이다', '▁입주', '▁입증', '▁입지', '▁입찰', '▁입학', '▁잇', '▁잇따라', '▁있', '▁있게', '▁있겠', '▁있고', '▁있기', '▁있느냐', '▁있는', '▁있는데', '▁있다', '▁있다고', '▁있다는', '▁있던', '▁있도록', '▁있습니다', '▁있어', '▁있어서', '▁있어야', '▁있었', '▁있었는데', '▁있었다', '▁있었던', '▁있었지만', '▁있으나', '▁있으니', '▁있으며', '▁있으면', '▁있을', '▁있을까', '▁있을지', '▁있음', '▁있지', '▁있지만', '▁자', '▁자격', '▁자극', '▁자금', '▁자기', '▁자녀', '▁자동', '▁자동차', '▁자랑', '▁자료', '▁자료를', '▁자리', '▁자리를', '▁자리에', '▁자리에서', '▁자발적', '▁자본', '▁자본시장', '▁자사', '▁자산', '▁자살', '▁자세', '▁자세한', '▁자신', '▁자신감', '▁자신들', '▁자신을', '▁자신의', '▁자신이', '▁자아냈다', '▁자연', '▁자연스럽게', '▁자원', '▁자원봉사', '▁자유', '▁자율', '▁자전거', '▁자존심', '▁자주', '▁자체', '▁자체가', '▁자칫', '▁자택', '▁자회사', '▁작', '▁작가', '▁작곡', '▁작년', '▁작동', '▁작성', '▁작업', '▁작업을', '▁작용', '▁작은', '▁작품', '▁잔', '▁잘', '▁잘못', '▁잘못된', '▁잠', '▁잠시', '▁잠실', '▁잠재', '▁잠정', '▁잡', '▁잡고', '▁잡아', '▁잡았다', '▁장', '▁장관', '▁장관은', '▁장기', '▁장난', '▁장르', '▁장면', '▁장비', '▁장소', '▁장애', '▁장애인', '▁장윤정', '▁장점', '▁장착', '▁장학금', '▁재', '▁재가동', '▁재개', '▁재건축', '▁재계약', '▁재난', '▁재능', '▁재무', '▁재미', '▁재미있', '▁재발', '▁재발방지', '▁재벌', '▁재산', '▁재원', '▁재정', '▁재정절벽', '▁재판', '▁재판부는', '▁재활', '▁쟁점', '▁저', '▁저녁', '▁저렴', '▁저소득층', '▁저장', '▁저지른', '▁적', '▁적극', '▁적극적으로', '▁적극적인', '▁적립', '▁적발', '▁적시타', '▁적어', '▁적용', '▁적은', '▁적응', '▁적이', '▁적자', '▁적절', '▁적지', '▁적합', '▁전', '▁전개', '▁전국', '▁전기', '▁전날', '▁전날보다', '▁전남', '▁전년', '▁전년대비', '▁전년동기', '▁전달', '▁전담', '▁전두환', '▁전략', '▁전력', '▁전망', '▁전망된다', '▁전망이다', '▁전망치', '▁전망했다', '▁전면', '▁전문', '▁전문가', '▁전문가들', '▁전문가들은', '▁전반', '▁전반기', '▁전부터', '▁전북', '▁전세', '▁전세계', '▁전시', '▁전액', '▁전에', '▁전역', '▁전용', '▁전원', '▁전자', '▁전쟁', '▁전주', '▁전지현', '▁전지훈련', '▁전체', '▁전체회의', '▁전통', '▁전투', '▁전파', '▁전해', '▁전해졌다', '▁전했다', '▁전혀', '▁전형', '▁전화', '▁전환', '▁전후', '▁절', '▁절감', '▁절대', '▁절반', '▁절차', '▁절차를', '▁젊은', '▁점', '▁점검', '▁점도', '▁점수', '▁점에서', '▁점유율', '▁점을', '▁점이', '▁점이다', '▁점점', '▁점차', '▁점포', '▁접', '▁접근', '▁접속', '▁접수', '▁접촉', '▁접한', '▁정', '▁정권', '▁정규', '▁정규직', '▁정기', '▁정당', '▁정당공천', '▁정도', '▁정도로', '▁정리', '▁정말', '▁정보', '▁정보를', '▁정보통신', '▁정부', '▁정부가', '▁정부는', '▁정부와', '▁정부의', '▁정부조직', '▁정비', '▁정상', '▁정상화', '▁정상회담', '▁정식', '▁정신', '▁정착', '▁정책', '▁정책을', '▁정체', '▁정치', '▁정치권', '▁정치적', '▁정확한', '▁정확히', '▁정황', '▁제', '▁제거', '▁제공', '▁제공하고', '▁제공하는', '▁제공한다', '▁제기', '▁제기되고', '▁제대로', '▁제도', '▁제목으로', '▁제시', '▁제시한', '▁제시했다', '▁제안', '▁제약', '▁제외', '▁제외한', '▁제임스', '▁제작', '▁제작발표회', '▁제작진', '▁제재', '▁제조', '▁제조업', '▁제조업체', '▁제주', '▁제주도', '▁제출', '▁제치고', '▁제품', '▁제품을', '▁제한', '▁제휴', '▁조', '▁조건', '▁조금', '▁조금씩', '▁조기', '▁조달', '▁조례', '▁조만간', '▁조명', '▁조사', '▁조사결과', '▁조사됐다', '▁조사를', '▁조사하고', '▁조선', '▁조성', '▁조성민', '▁조심', '▁조언', '▁조율', '▁조작', '▁조절', '▁조정', '▁조종사', '▁조직', '▁조직개편', '▁조치', '▁조치를', '▁조합원', '▁조항', '▁존', '▁존재', '▁존중', '▁졸업', '▁좀', '▁종', '▁종교', '▁종로구', '▁종료', '▁종류', '▁종목', '▁종합', '▁좋', '▁좋겠다', '▁좋다', '▁좋아', '▁좋았', '▁좋은', '▁좋지', '▁좌', '▁죄', '▁주', '▁주가', '▁주가가', '▁주거', '▁주고', '▁주관', '▁주는', '▁주도', '▁주력', '▁주로', '▁주말', '▁주말드라마', '▁주목', '▁주목된다', '▁주문', '▁주민', '▁주변', '▁주식', '▁주식시장', '▁주연', '▁주요', '▁주요뉴스', '▁주인공', '▁주장', '▁주장했다', '▁주재', '▁주제', '▁주제로', '▁주주', '▁주최', '▁주택', '▁죽', '▁죽음', '▁준', '▁준공', '▁준다', '▁준비', '▁준수', '▁준우승', '▁줄', '▁줄어든', '▁줄어들', '▁줄었다', '▁줄여', '▁중', '▁중간', '▁중국', '▁중국의', '▁중단', '▁중반', '▁중소', '▁중소기업', '▁중소형주', '▁중순', '▁중심', '▁중심으로', '▁중앙', '▁중요', '▁중요성', '▁중요하다', '▁중요한', '▁중이다', '▁중인', '▁중장기', '▁중점', '▁중흥', '▁즉', '▁즉각', '▁즉시', '▁즐거운', '▁즐기', '▁즐길', '▁증', '▁증가', '▁증가율', '▁증가한', '▁증가했다', '▁증거', '▁증권', '▁증권사', '▁증명', '▁증시', '▁증액', '▁증인', '▁지', '▁지구', '▁지금', '▁지금까지', '▁지금은', '▁지급', '▁지나', '▁지난', '▁지난달', '▁지난해', '▁지내', '▁지낸', '▁지닌', '▁지도', '▁지도부', '▁지도자', '▁지동원', '▁지명', '▁지목', '▁지방', '▁지방자치단체', '▁지배', '▁지분', '▁지사', '▁지속', '▁지속될', '▁지속적으로', '▁지속적인', '▁지수', '▁지시', '▁지식', '▁지식경제부', '▁지역', '▁지연', '▁지원', '▁지원을', '▁지원하는', '▁지원한다', '▁지자체', '▁지적', '▁지적했다', '▁지점', '▁지정', '▁지지', '▁지출', '▁지켜', '▁지켜보', '▁지키', '▁지표', '▁지하', '▁지하철', '▁지휘', '▁직', '▁직구', '▁직무', '▁직업', '▁직원', '▁직원들', '▁직장', '▁직장인', '▁직전', '▁직접', '▁직후', '▁진', '▁진단', '▁진료', '▁진보', '▁진술', '▁진실', '▁진입', '▁진주의료원', '▁진짜', '▁진출', '▁진행', '▁진행됐다', '▁진행되는', '▁진행된', '▁진행된다', '▁진행될', '▁진행하고', '▁진행한다', '▁진화', '▁질', '▁질문', '▁질문에', '▁질병', '▁짐', '▁집', '▁집계', '▁집계됐다', '▁집권', '▁집단', '▁집중', '▁집행', '▁집행유예', '▁집회', '▁짓', '▁징계', '▁징역', '▁짜', '▁짧은', '▁쪽', '▁쪽으로', '▁찍', '▁찍은', '▁차', '▁차관', '▁차기', '▁차단', '▁차량', '▁차례', '▁차별', '▁차별화된', '▁차세대', '▁차원에서', '▁차원의', '▁차이', '▁차이가', '▁차지', '▁차지하는', '▁차지한', '▁차지했다', '▁차질', '▁착', '▁착공', '▁착륙', '▁착수', '▁착용', '▁찬', '▁찬성', '▁참', '▁참가', '▁참가자들', '▁참고', '▁참석', '▁참석한', '▁참석해', '▁참석했다', '▁참여', '▁참여한', '▁참의원', '▁창', '▁창단', '▁창업', '▁창원', '▁창조', '▁창출', '▁찾기', '▁찾는', '▁찾아', '▁찾았다', '▁찾은', '▁찾을', '▁찾지', '▁채', '▁채권', '▁채널', '▁채무', '▁채용', '▁채택', '▁책', '▁책임', '▁책임을', '▁챔피언십', '▁챙겨', '▁챙기', '▁챙긴', '▁처', '▁처리', '▁처벌', '▁처분', '▁처음', '▁처음으로', '▁처음이다', '▁천', '▁천안', '▁철', '▁철강', '▁철거', '▁철저한', '▁철저히', '▁철학', '▁철회', '▁첨단', '▁첫', '▁첫날', '▁청', '▁청구', '▁청년', '▁청문회', '▁청소년', '▁청약', '▁청와대', '▁청주', '▁체', '▁체결', '▁체결했다', '▁체계적', '▁체력', '▁체제', '▁체크', '▁체포', '▁체험', '▁첼시', '▁초', '▁초과', '▁초기', '▁초대', '▁초등학교', '▁초반', '▁초점을', '▁초청', '▁촉구', '▁촉구했다', '▁촉진', '▁총', '▁총괄', '▁총기', '▁총리', '▁총장', '▁총재', '▁촬영', '▁최', '▁최강', '▁최강희', '▁최고', '▁최고위원', '▁최고의', '▁최근', '▁최다', '▁최대', '▁최대주주', '▁최대한', '▁최선을', '▁최소', '▁최소화', '▁최신', '▁최악', '▁최우선', '▁최우수', '▁최저', '▁최종', '▁최초', '▁최초로', '▁추', '▁추가', '▁추가로', '▁추격', '▁추구', '▁추락', '▁추산', '▁추세', '▁추신수', '▁추신수는', '▁추억', '▁추적', '▁추정', '▁추정된다', '▁추진', '▁추진하고', '▁추진할', '▁추징금', '▁추천', '▁추첨', '▁축', '▁축구', '▁축구협회', '▁축소', '▁축제', '▁축하', '▁춘천', '▁출', '▁출구전략', '▁출국', '▁출근', '▁출동', '▁출발', '▁출범', '▁출산', '▁출석', '▁출시', '▁출신', '▁출연', '▁출연한', '▁출연해', '▁출연했다', '▁출입', '▁출장', '▁출전', '▁출처', '▁출판', '▁충', '▁충격', '▁충남', '▁충돌', '▁충북', '▁충분', '▁충분히', '▁충실', '▁충족', '▁충청', '▁취', '▁취급', '▁취득', '▁취득세', '▁취소', '▁취약', '▁취업', '▁취임', '▁취임식', '▁취재', '▁취재진', '▁취지', '▁취하고', '▁측', '▁측근', '▁측면', '▁측면에서', '▁측은', '▁측정', '▁치', '▁치러', '▁치료', '▁치료를', '▁치르', '▁치른', '▁치솟', '▁치열', '▁치열한', '▁친', '▁친구', '▁친구들', '▁친환경', '▁침', '▁침수', '▁침체', '▁침해', '▁칭찬', '▁카', '▁카드', '▁카드사', '▁카리스마', '▁카메라', '▁카카오', '▁카페', '▁칼', '▁캐', '▁캐나다', '▁캐릭터', '▁캐스팅', '▁캘리포니아', '▁캠페인', '▁캠프', '▁캠핑', '▁캡처', '▁커', '▁커뮤니티', '▁커지고', '▁커플', '▁커피', '▁컨트롤', '▁컬러', '▁컴백', '▁컴퓨터', '▁컸다', '▁케', '▁케이', '▁케이블', '▁코', '▁코너', '▁코넥스', '▁코리아', '▁코미디', '▁코스', '▁코스닥', '▁코스피', '▁코스피지수', '▁코치', '▁콘', '▁콘서트', '▁콘셉트', '▁콘텐츠', '▁콜', '▁쿠', '▁크', '▁크게', '▁크기', '▁크다', '▁크로스', '▁크루즈', '▁크리스', '▁큰', '▁클', '▁클라라', '▁클래식', '▁클럽', '▁키', '▁키스', '▁키우', '▁키워', '▁키워드', '▁타', '▁타격', '▁타고', '▁타선', '▁타율', '▁타이', '▁타이틀', '▁타이틀곡', '▁타자', '▁탄', '▁탄력', '▁탄생', '▁탄탄한', '▁탈', '▁탈락', '▁탈출', '▁탐', '▁탑승', '▁탑재', '▁탓', '▁탓에', '▁탔다', '▁태', '▁태국', '▁태도', '▁태블릿', '▁태양', '▁태양광', '▁태어났다', '▁택시', '▁터', '▁터치', '▁터키', '▁털어놓', '▁털어놨다', '▁테', '▁테러', '▁테마', '▁테스트', '▁토', '▁토대로', '▁토론회', '▁토지', '▁토크쇼', '▁톱스타', '▁통', '▁통계', '▁통과', '▁통보', '▁통산', '▁통상', '▁통신', '▁통일', '▁통일부', '▁통제', '▁통증', '▁통한', '▁통합', '▁통해', '▁통행', '▁통화', '▁통화정책', '▁퇴', '▁퇴직', '▁투', '▁투구', '▁투명', '▁투수', '▁투어', '▁투입', '▁투자', '▁투자의견', '▁투자자', '▁투자자들', '▁투표', '▁트', '▁트렌드', '▁트위터', '▁트위터에', '▁특', '▁특별', '▁특별사면', '▁특별한', '▁특사', '▁특성', '▁특수', '▁특위', '▁특유', '▁특정', '▁특집', '▁특징', '▁특징이다', '▁특허', '▁특혜', '▁특히', '▁틀', '▁티아라', '▁티저', '▁티켓', '▁팀', '▁팀의', '▁파', '▁파견', '▁파괴', '▁파악', '▁파워', '▁파트너', '▁판', '▁판결', '▁판단', '▁판단했다', '▁판매', '▁판매량', '▁판문점', '▁판사', '▁판정', '▁팔', '▁패', '▁패널', '▁패배', '▁패션', '▁패스', '▁패키지', '▁팬', '▁팬들', '▁팬들에게', '▁팬들의', '▁팽팽', '▁퍼', '▁퍼포먼스', '▁펀드', '▁페', '▁페널티', '▁페이스북', '▁편', '▁편리하게', '▁편성', '▁편의점', '▁편집', '▁펼쳐', '▁펼쳤다', '▁펼치고', '▁펼칠', '▁평', '▁평가', '▁평가를', '▁평가했다', '▁평균', '▁평균자책점', '▁평생', '▁평소', '▁평창', '▁평택', '▁평화', '▁폐', '▁폐기', '▁폐쇄', '▁폐지', '▁포', '▁포기', '▁포스코', '▁포인트', '▁포즈를', '▁포착', '▁포털', '▁포토', '▁포함', '▁포함돼', '▁포함됐다', '▁포함된', '▁포함한', '▁포함해', '▁포항', '▁폭', '▁폭력', '▁폭로', '▁폭발', '▁폭염', '▁폭우', '▁폭풍', '▁폭행', '▁폴', '▁표', '▁표명', '▁표시', '▁표정', '▁표준', '▁표현', '▁푸', '▁푸이그', '▁풀', '▁풀어', '▁풀이된다', '▁품', '▁품목', '▁품질', '▁풍', '▁풍부한', '▁프랑스', '▁프로', '▁프로그램', '▁프로그램을', '▁프로모션', '▁프로야구', '▁프로젝트', '▁프로축구', '▁프리', '▁프리미어리그', '▁프리미엄', '▁플랫폼', '▁플레이', '▁피', '▁피부', '▁피의자', '▁피해', '▁피해가', '▁피해를', '▁피해자', '▁필', '▁필리핀', '▁필수', '▁필요', '▁필요가', '▁필요성', '▁필요하다', '▁필요한', '▁하', '▁하겠다', '▁하고', '▁하기', '▁하나', '▁하나로', '▁하는', '▁하는데', '▁하다', '▁하락', '▁하락세', '▁하락한', '▁하락했다', '▁하루', '▁하며', '▁하면', '▁하면서', '▁하반기', '▁하이', '▁하자', '▁하정우', '▁하지', '▁하지만', '▁하향', '▁학', '▁학교', '▁학교폭력', '▁학부모', '▁학생', '▁학생들', '▁학습', '▁한', '▁한강', '▁한계', '▁한국', '▁한국거래소', '▁한국은', '▁한국은행', '▁한국의', '▁한국인', '▁한국전력', '▁한다', '▁한다고', '▁한다는', '▁한때', '▁한마디', '▁한반도', '▁한번', '▁한층', '▁한파', '▁한편', '▁한혜진', '▁한화', '▁할', '▁할리우드', '▁할머니', '▁할부', '▁할인', '▁함', '▁함께', '▁합', '▁합격', '▁합니다', '▁합동', '▁합류', '▁합리적', '▁합병', '▁합의', '▁항', '▁항공', '▁항공기', '▁항목', '▁항상', '▁항소심', '▁해', '▁해결', '▁해당', '▁해당하는', '▁해도', '▁해명', '▁해명했다', '▁해병대', '▁해상', '▁해서', '▁해석', '▁해소', '▁해야', '▁해양', '▁해외', '▁해운대', '▁핵', '▁핵실험', '▁핵심', '▁했', '▁했는데', '▁했다', '▁했던', '▁했지만', '▁행', '▁행동', '▁행보', '▁행복', '▁행사', '▁행사를', '▁행사에', '▁행위', '▁행정', '▁행진', '▁향', '▁향상', '▁향한', '▁향해', '▁향후', '▁허', '▁허가', '▁허리', '▁허용', '▁허위', '▁헌', '▁헌법', '▁헌법재판소장', '▁헌재', '▁헤', '▁헤어', '▁혁신', '▁현', '▁현금', '▁현대', '▁현대건설', '▁현대모비스', '▁현대자동차', '▁현대중공업', '▁현대차', '▁현대캐피탈', '▁현상', '▁현실', '▁현안', '▁현역', '▁현장', '▁현장에서', '▁현재', '▁현재까지', '▁현지', '▁현행', '▁현황', '▁혈', '▁혐의', '▁혐의로', '▁혐의를', '▁협력', '▁협력업체', '▁협박', '▁협상', '▁협약', '▁협업', '▁협의', '▁협조', '▁형', '▁형사', '▁형성', '▁형식', '▁형제', '▁형태', '▁형태로', '▁혜택', '▁혜택을', '▁호', '▁호소', '▁호응', '▁호조', '▁호주', '▁호텔', '▁호투', '▁호평', '▁호흡', '▁혹은', '▁혼', '▁혼란', '▁혼자', '▁홀', '▁홈', '▁홈경기', '▁홈런', '▁홈페이지', '▁홍', '▁홍명보', '▁홍보', '▁홍콩', '▁화', '▁화려한', '▁화면', '▁화보', '▁화성', '▁화이트', '▁화장실', '▁화장품', '▁화재', '▁화제', '▁화제다', '▁화학', '▁확', '▁확대', '▁확보', '▁확산', '▁확실한', '▁확실히', '▁확인', '▁확인됐다', '▁확인할', '▁확장', '▁확정', '▁확충', '▁환', '▁환경', '▁환영', '▁환율', '▁환자', '▁활동', '▁활동을', '▁활발', '▁활성화', '▁활약', '▁활용', '▁활용해', '▁활주로', '▁황', '▁황금', '▁황우여', '▁회', '▁회계', '▁회담', '▁회복', '▁회사', '▁회사채', '▁회원', '▁회원가입', '▁회의', '▁회의록', '▁회의를', '▁회장', '▁회장은', '▁회장의', '▁회장이', '▁획득', '▁횡령', '▁효', '▁효과', '▁효과가', '▁효과를', '▁효과적', '▁효율성', '▁효율적', '▁후', '▁후문이다', '▁후반', '▁후반기', '▁후보', '▁후보자', '▁후속', '▁후원', '▁훈련', '▁훌륭', '▁훔친', '▁훨씬', '▁훼손', '▁휘', '▁휩쓸', '▁휴', '▁휴가', '▁휴대전화', '▁휴대폰', '▁휴식', '▁흐', '▁흐름', '▁흑자', '▁흔들', '▁흔적', '▁흘러', '▁흘리', '▁흡수', '▁흡연', '▁흥국생명', '▁흥미', '▁흥행', '▁희망', '▁희생', '▁히트', '▁힘', '▁힘든', '▁힘들', '▁힘을', '▁힘입어', '▁힙합', '■', '□', '▦', '▲', '△', '▶', '▷', '▼', '▽', '◀', '◆', '◇', '◈', '○', '★', '☎', '〃', '〈', '〉', '「', '」', '『', '』', '【', '】', '一', '三', '上', '下', '不', '中', '亞', '京', '人', '企', '倍', '先', '公', '前', '北', '南', '反', '史', '國', '報', '外', '大', '天', '女', '子', '安', '家', '對', '小', '山', '島', '州', '市', '平', '年', '弗', '心', '性', '故', '文', '新', '日', '晋', '朴', '李', '東', '株', '檢', '母', '比', '民', '江', '海', '無', '獨', '王', '生', '田', '甲', '男', '百', '盧', '知', '硏', '社', '美', '習', '胎', '與', '英', '草', '行', '西', '證', '車', '軍', '近', '道', '重', '野', '金', '銀', '電', '靑', '非', '韓', '高', '鬼', '가', '가격', '가구', '가량', '가족', '가지', '각', '간', '간다', '갇', '갈', '감', '감독', '감사', '감을', '갑', '값', '갓', '갔', '갔다', '강', '강남스타일', '강심장', '갖', '같', '같은', '갚', '개', '개국', '개그콘서트', '개로', '개를', '개발', '개사', '개선', '개성공단', '개월', '개월간', '개의', '개혁', '객', '갤', '갭', '갯', '갱', '갸', '걀', '거', '거나', '거래', '거래소', '거래일', '거리', '걱', '건', '건강', '건설', '건으로', '건전성', '걷', '걸', '걸음', '검', '검사', '검색', '검증', '검찰', '겁', '것', '겉', '게', '게임', '겐', '겔', '겟', '겠', '겠다', '겠다고', '겠다는', '겠습니다', '겠지만', '겨', '격', '겪', '견', '결', '결과', '결정', '결제', '결혼', '겸', '겹', '겼', '겼다', '경', '경기', '경기에서', '경영', '경쟁', '경쟁력', '경제', '경찰', '경찰서', '경찰서는', '경찰청', '곁', '계', '계약', '계획', '고', '고객', '고등학교', '고속도로', '곡', '곤', '곧', '골', '골을', '골프', '곰', '곱', '곳', '공', '공간', '공개', '공급', '공단', '공동체', '공무원', '공사', '공약', '공업', '공연', '공원', '공장', '공항', '공화국', '곶', '과', '과정', '과정에서', '과학', '과학기술', '곽', '관', '관계', '관광', '관련', '관리', '괄', '괌', '광', '광고', '광역시', '괜', '괴', '괴물', '굉', '교', '교사', '교섭', '교육', '교육청', '교통', '교회', '구', '구나', '구단', '구를', '구역', '구장에서', '구조', '구청장', '국', '국가', '국내', '국내외', '국민', '국장', '국정원', '국제', '국회', '군', '군은', '군의', '굳', '굴', '굵', '굶', '굽', '굿', '궁', '궂', '궈', '권', '권을', '궐', '궜', '궤', '귀', '귀태', '귄', '규', '규모', '규제', '규칙', '균', '귤', '그', '그동안', '그래', '그런', '그룹', '극', '극본', '극장', '근', '글', '글로벌', '긁', '금', '금리', '금속', '금액', '금융', '금융그룹', '금융지주', '금을', '급', '긋', '긍', '기', '기가', '기간', '기관', '기구', '기금', '기념', '기는', '기능', '기도', '기로', '기록', '기를', '기사', '기술', '기아차', '기업', '기업은행', '기에', '기자', '기준', '기지', '기획', '긴', '길', '김', '깁', '깃', '깅', '깊', '까', '까지', '깍', '깎', '깐', '깔', '깜', '깜찍', '깝', '깡', '깥', '깨', '깬', '꺼', '꺾', '껄', '껌', '껍', '껏', '껑', '께', '께서', '껴', '꼈', '꼬', '꼭', '꼴', '꼼', '꼽', '꽁', '꽂', '꽃', '꽃보다', '꽉', '꽝', '꽤', '꾀', '꾸', '꾼', '꿀', '꿇', '꿈', '꿋', '꿔', '꿨', '꿰', '뀌', '뀐', '뀔', '끄', '끄러', '끈', '끊', '끌', '끓', '끔', '끗', '끝', '끼', '끼리', '끽', '낀', '낄', '낌', '나', '나갈', '나갔다', '나는', '나라', '나무', '낙', '낚', '난', '날', '낡', '남', '남북', '남자', '납', '낫', '났', '났다', '낭', '낮', '낯', '낱', '낳', '내', '내가', '내기', '낵', '낸', '낼', '냄', '냅', '냈', '냈다', '냉', '냐', '냥', '너', '너목들', '넉', '넋', '넌', '널', '넓', '넘', '넛', '넝', '넣', '네', '네요', '네트워크', '넥', '넨', '넬', '넷', '넸', '녀', '녁', '년', '년간', '년까지', '년대', '년만에', '년부터', '년생', '년에', '년에는', '년째', '념', '녔', '녕', '노', '노동', '노조', '노컷', '노트', '녹', '논', '놀', '놈', '농', '농협', '높', '놓', '놓고', '놔', '놨', '뇌', '뇨', '누', '눅', '눈', '눌', '눔', '눕', '눠', '눴', '뉘', '뉜', '뉴', '뉴스', '뉴시스', '늄', '느', '느냐', '늑', '는', '는데', '늘', '늙', '늠', '능', '능력', '늦', '늪', '늬', '니', '니까', '니다', '니스', '니아', '닉', '닌', '닐', '님', '닙', '닛', '닝', '다', '다른', '다면', '다운', '닥', '닦', '단', '단계', '단지', '단체', '닫', '달', '달라', '달러', '닭', '닮', '담', '담당', '담보대출', '답', '닷', '닷컴', '당', '당국', '당선인', '닿', '대', '대가', '대강', '대교', '대로', '대를', '대비', '대사관', '대상', '대우증권', '대의', '대책', '대출', '대통령', '대표', '대표팀', '대학', '대학교', '대학원', '대한', '대화록', '대회', '댁', '댄', '댈', '댐', '댓', '댔', '댜', '더', '더니', '더라', '더라도', '덕', '던', '덜', '덟', '덤', '덥', '덧', '덩', '덮', '데', '데다', '데일리', '덱', '덴', '델', '뎀', '뎁', '뎌', '도', '도로', '도록', '도서관', '도시', '도지사', '독', '돈', '돋', '돌', '돔', '돕', '돗', '동', '동맹', '동안', '동향', '돼', '돼야', '됐', '됐고', '됐다', '됐던', '됐습니다', '됐으나', '됐으며', '됐지만', '되', '되고', '되기', '되는', '되도록', '되며', '되면', '되면서', '되어', '되었', '되었다', '되자', '되지', '된', '된다', '된다면', '될', '됨', '됨에', '됩', '됩니다', '두', '둑', '둔', '둘', '둠', '둡', '둥', '둬', '뒀', '뒤', '뒷', '듀', '듈', '드', '드는', '드라마', '드래곤', '드리', '드립니다', '득', '득점', '든', '든지', '듣', '들', '들과', '들도', '들어', '들에', '들에게', '들은', '들을', '들의', '들이', '듬', '듭', '듯', '등', '등급', '등록', '디', '디스크', '디스플레이', '디지털', '딕', '딘', '딛', '딜', '딤', '딧', '딩', '딪', '따', '딱', '딴', '딸', '땀', '땅', '때', '땐', '땠', '땡', '떠', '떡', '떤', '떨', '떳', '떴', '떻', '떼', '뗀', '뗄', '뗐', '또', '똑', '똘', '똥', '뚜', '뚝', '뚫', '뚱', '뛰', '뛴', '뛸', '뜨', '뜩', '뜬', '뜯', '뜰', '뜸', '뜻', '띄', '띈', '띔', '띠', '띤', '라', '라고', '라는', '라도', '라디오스타', '라며', '라면', '라운드', '라이', '라이트', '라이프', '라인', '락', '란', '랄', '람', '랍', '랏', '랐', '랑', '래', '랙', '랜', '랜드', '랠', '램', '랩', '랫', '랬', '랭', '랴', '략', '량', '량이', '러', '러스', '럭', '런', '런닝맨', '럴', '럼', '럽', '럿', '렀', '렀다', '렁', '렇', '레', '레드', '레미제라블', '레스', '레이', '렉', '렌', '렐', '렘', '렛', '려', '려고', '려는', '려면', '력', '력을', '력이', '련', '렬', '렴', '렵', '렷', '렸', '렸고', '렸다', '렸던', '렸지만', '령', '례', '로', '로부터', '로서', '로운', '록', '론', '롤', '롬', '롭', '롭게', '롯', '롯데', '롱', '뢰', '료', '룡', '루', '루타', '룩', '룬', '룰', '룸', '룹', '룻', '룽', '뤄', '뤘', '뤼', '류', '륙', '륜', '률', '륨', '륭', '르', '르트', '른', '를', '름', '릅', '릇', '릉', '릎', '리', '리그', '리는', '리를', '리바운드', '리스', '리스트', '리아', '리조트', '리지', '릭', '린', '린다', '릴', '림', '립', '릿', '링', '링크', '마', '마다', '마을', '마음', '마저', '마케팅', '마켓', '마트', '막', '만', '만달러', '만명', '만원', '만원으로', '만원을', '만으로', '만큼', '많', '맏', '말', '맑', '맘', '맙', '맛', '망', '맞', '맡', '매', '매매', '매출', '매치', '맥', '맨', '맴', '맵', '맷', '맹', '맺', '머', '머니', '머니투데이', '머리', '먹', '먼', '먼트', '멀', '멈', '멋', '멍', '메', '메이', '멕', '멘', '멜', '멤', '멧', '며', '면', '면서', '면서도', '면세점', '멸', '몄', '명', '명과', '명에게', '명으로', '명은', '명을', '명의', '명이', '몇', '모', '모델', '모바일', '모씨', '목', '목표', '몫', '몬', '몰', '몸', '몹', '못', '몽', '뫼', '묘', '무', '무릎팍도사', '무역', '무한도전', '묵', '묶', '문', '문을', '문제', '문학', '문화', '문화예술', '묻', '물', '물을', '물질', '뭄', '뭇', '뭉', '뭐', '뭔', '뭘', '뮌', '뮤', '뮤직', '뮬', '므', '미', '미국', '미디어', '미래', '미술관', '미스터', '믹', '믹스', '민', '민주', '민주당', '믿', '밀', '밋', '밌', '밍', '및', '밑', '바', '바다', '바람', '바르셀로나', '바이', '바이오', '바퀴', '박', '박근혜', '박람회', '박물관', '밖', '밖에', '반', '반도체', '받', '받고', '받는', '받아', '받았다', '받은', '받을', '발', '발굴', '발언', '발전', '발전소', '발표', '밝', '밟', '밤', '밥', '방', '방송', '방식', '방안', '방위', '방향', '밭', '배', '배우', '백', '백화점', '밴', '밴드', '밸', '뱀', '뱃', '뱅', '뱉', '버', '버리', '버린', '버스', '벅', '번', '번째', '번호', '번홀', '벌', '범', '범죄', '법', '법원', '법을', '법인', '벗', '벙', '베', '베르', '베를린', '베이', '베이스', '벡', '벤', '벤처', '벨', '벨트', '벳', '벵', '벼', '벽', '변', '별', '별로', '별로는', '볍', '볐', '병', '병원', '볕', '보', '보건', '보고', '보고서', '보기', '보는', '보니', '보다', '보다는', '보드', '보이', '보장', '보증', '보험', '보호', '복', '복지', '복합', '볶', '본', '본부', '본부장', '볼', '볼넷', '볼륨', '봄', '봅', '봇', '봉', '봐', '봐야', '봤', '봤다', '뵙', '부', '부가', '부는', '부담', '부동산', '부르크', '부문', '부산', '부장', '부장검사', '부장판사', '부처', '부터', '부품', '북', '북방한계선', '북부', '북한', '분', '분과', '분기', '분기에', '분께', '분석', '분야', '분쯤', '불', '붉', '붐', '붓', '붕', '붙', '뷔', '뷰', '브', '브라', '브랜드', '브레이크', '브리', '븐', '블', '블랙', '블록', '비', '비가', '비를', '비서관', '비아', '비용', '비율', '비치', '빅', '빈', '빌', '빌딩', '빔', '빕', '빗', '빙', '빚', '빛', '빠', '빡', '빨', '빴', '빵', '빼', '빽', '뺀', '뺏', '뺐', '뺑', '뺨', '뻐', '뻑', '뻔', '뻗', '뻤', '뻥', '뼈', '뽀', '뽐', '뽑', '뽕', '뾰', '뿌', '뿐', '뿐만', '뿔', '뿜', '쁘', '쁜', '쁠', '쁨', '삐', '사', '사가', '사건', '사고', '사는', '사람', '사랑', '사를', '사무소', '사실', '사업', '사업본부', '사업부', '사업자', '사와', '사의', '사이드', '사이트', '사진', '사진제공', '사항', '사회', '삭', '산', '산업', '산업단지', '살', '삶', '삼', '삼성', '삼성전자', '삽', '삿', '샀', '상', '상공회의소', '상담', '상당히', '상을', '상태', '상품', '상황', '샅', '새', '새누리당', '색', '샌', '샐', '샘', '생', '생명', '생산', '생활', '샤', '샬', '샴', '샵', '샷', '샹', '섀', '서', '서는', '서비스', '서울', '석', '섞', '선', '선거', '선물', '선수', '선수권대회', '선을', '섣', '설', '설국열차', '설명회', '섬', '섭', '섯', '섰', '성', '성과', '성은', '성을', '성이', '성장', '세', '세가', '세계', '세계일보', '세대', '세력', '세를', '세요', '세이브', '세트', '세포', '섹', '센', '센스', '센터', '센터에서', '센트', '셀', '셈', '셉', '셋', '셍', '셔', '션', '셜', '셨', '셰', '셸', '소', '소득', '소리', '소방서', '소비자', '소송', '소연', '소프트', '속', '손', '손해보험', '솔', '솜', '솟', '송', '솥', '쇄', '쇠', '쇼', '숀', '숍', '숏', '수', '수가', '수는', '수록', '수를', '수사', '수석', '수수료', '수술', '수원', '수익', '수익률', '수지', '숙', '순', '순위', '술', '숨', '숨어있', '숫', '숭', '숱', '숲', '쉐', '쉬', '쉰', '쉴', '쉼', '쉽', '슈', '슈퍼', '슈퍼스타', '슐', '슘', '슛', '스', '스가', '스는', '스러운', '스러워', '스럽', '스럽게', '스럽다', '스를', '스마트', '스와', '스완지시티', '스의', '스카', '스캔들', '스케', '스코', '스쿨', '스크', '스키', '스타', '스탠', '스터', '스턴', '스테', '스토리', '스토어', '스트', '스티', '스페셜', '스포츠', '스포츠조선', '슨', '슬', '슴', '습', '습니까', '습니다', '슷', '승', '승을', '시', '시간', '시께', '시대', '시리즈', '시민', '시부터', '시설', '시스템', '시아', '시의회', '시장', '시장에서', '시즌', '시청', '시켜', '시켰다', '시키', '시키고', '시키기', '시키는', '시킨', '시킬', '시티', '시험', '식', '식을', '식품', '신', '신도시', '신문', '신청', '싣', '실', '실리콘', '실에서', '실장', '실적', '실점', '싫', '심', '심리', '심사', '심을', '십', '싱', '싶', '싸', '싸움', '싹', '싼', '쌀', '쌈', '쌌', '쌍', '쌓', '써', '썩', '썬', '썰', '썸', '썹', '썼', '썽', '쎄', '쏘', '쏙', '쏜', '쏟', '쏠', '쏴', '쐐', '쑤', '쑥', '쓰', '쓴', '쓸', '씀', '씁', '씌', '씨', '씨가', '씨는', '씨를', '씨에게', '씨와', '씨의', '씩', '씬', '씹', '씻', '씽', '아', '아버지', '아빠', '아시아', '아시아경제', '아시아투데이', '아웃', '아이', '아일랜드', '아직', '아카데미', '아트', '아파트', '아프리카', '악', '안', '안드로이드', '안보', '안을', '안전', '안정', '안타', '앉', '않', '알', '알리미', '앓', '암', '압', '앗', '았', '았다', '았던', '았지만', '앙', '앞', '앞으로', '애', '액', '액은', '앤', '앨', '앰', '앱', '앳', '앵', '앵커멘트', '야', '야구', '약', '약품', '얀', '얄', '얇', '얌', '얏', '양', '얘', '어', '어야', '어요', '억', '억달러', '억여원', '억원', '억원으로', '억원을', '억원의', '언', '언더파', '언론', '얹', '얻', '얼', '얽', '엄', '업', '업계', '업무', '업소', '업자', '업종', '업체', '업체들', '없', '없는', '없이', '엇', '었', '었고', '었는데', '었다', '었던', '었습니다', '었으나', '었으며', '었지만', '엉', '엎', '에', '에게', '에게는', '에너지', '에는', '에도', '에만', '에서', '에서는', '에서도', '에선', '엑', '엑스', '엔', '엔지니어링', '엔터테인먼트', '엘', '엠', '엣', '엥', '여', '여개', '여건', '여름', '여명', '여명의', '여명이', '여성', '여자', '여행', '역', '엮', '연', '연구', '연구소', '연구원', '연금', '연맹', '연수원', '연승', '연예', '연패', '연합', '연합회', '열', '염', '엽', '엿', '였', '였고', '였다', '였던', '였습니다', '였으며', '였지만', '영', '영상', '영업', '영화', '영화제', '옆', '예', '예방', '예산', '예술', '옌', '옐', '옛', '오', '오는', '오늘', '오른쪽', '오토', '오픈', '옥', '온', '온라인', '올', '올림픽', '올해', '옮', '옳', '옴', '옵', '옵션', '옷', '옹', '와', '와의', '완', '왈', '왑', '왓', '왔', '왔다', '왔던', '왕', '왜', '왠', '외', '외국인', '왼', '왼쪽', '요', '요금', '욕', '욘', '용', '용품', '우', '우는', '우리', '우스', '욱', '운', '운동', '운영', '울', '움', '웃', '웃음', '웅', '워', '웍', '원', '원에', '원에서', '원으로', '원은', '원을', '원의', '원이', '월', '월까지', '월드', '월드컵', '월말', '월부터', '월에', '월에는', '웠', '웠다', '웨', '웨어', '웨이', '웬', '웰', '웹', '위', '위권', '위기', '위는', '위로', '위를', '위안', '위에', '위원', '위원장', '위원회', '위원회는', '위험', '윅', '윈', '윌', '윔', '윗', '윙', '유', '유럽', '유통', '유플러스', '육', '윤', '율', '율은', '율을', '율이', '융', '으', '으나', '으니', '으려', '으로', '으로부터', '으로서', '으로써', '으며', '으면', '으면서', '윽', '은', '은행', '을', '음', '음악', '음에도', '음을', '읍', '응', '의', '이', '이기도', '이나', '이닝', '이다', '이라', '이라고', '이라는', '이라도', '이라며', '이라면서', '이란', '이며', '이번', '이상', '이어서', '이었다', '이었던', '이익', '이자', '이지만', '이하', '익', '인', '인데', '인사', '인수위', '인지', '인터내셔널', '인터넷', '일', '일간', '일까지', '일반', '일보', '일본', '일부터', '일에는', '읽', '잃', '임', '임을', '입', '입니다', '잇', '있', '있는', '있다', '잉', '잊', '잎', '자', '자가', '자금', '자는', '자동차', '자들', '자들은', '자들의', '자들이', '자로', '자료', '자를', '자리', '자마자', '자본', '자산', '자산운용', '자에게', '자와', '자원', '자의', '자인', '자치', '작', '작업', '잔', '잖', '잘', '잠', '잡', '잣', '장', '장과', '장관', '장비', '장애', '장애인', '장에서', '장으로', '장은', '장을', '장이', '장치', '잦', '재', '재단', '재료', '재산', '재정', '재판', '잭', '쟁', '저', '저축', '저축은행', '적', '적으로', '적이고', '적이다', '적인', '전', '전략', '전문', '전에', '전에서', '전우치', '전을', '전자', '전쟁', '전환', '절', '절차', '젊', '점', '점검', '점으로', '점을', '점이', '접', '젓', '정', '정글의', '정보', '정부', '정책', '정치', '젖', '제', '제도', '제를', '제약', '제주', '제품', '젝', '젠', '젤', '젬', '젯', '져', '져야', '젼', '졌', '졌고', '졌다', '졌습니다', '졌지만', '조', '조건', '조사', '조선', '조선해양', '조원', '조정', '조직', '조차', '조치', '족', '존', '졸', '좀', '좁', '종', '종목', '종합', '좋', '좋은', '좌', '죄', '죠', '주', '주가', '주기', '주년', '주는', '주민', '주세요', '주식', '주의', '주의보', '주택', '죽', '준', '준비', '준호', '준희', '줄', '줌', '줍', '중', '중견기업', '중공업', '중국', '중소기업', '중심', '중앙', '중인', '줘', '줘야', '줬', '줬다', '쥐', '쥔', '쥬', '즈', '즉', '즌', '즐', '즘', '즙', '증', '증권', '지', '지가', '지검', '지고', '지구', '지금', '지나치게', '지난', '지난해', '지는', '지도', '지를', '지만', '지면서', '지방', '지방경찰청', '지법', '지수', '지수는', '지역', '지원', '지원센터', '지지', '지훈', '직', '직원', '진', '진다', '진영', '진짜', '진흥', '진흥원', '질', '질서', '질환', '짐', '집', '집행', '짓', '징', '짖', '짙', '짚', '짜', '짜리', '짝', '짠', '짤', '짧', '짬', '짱', '째', '쨌', '쩌', '쩍', '쩔', '쩡', '쪼', '쪽', '쫄', '쫓', '쭈', '쭉', '쯔', '쯤', '찌', '찍', '찐', '찔', '찜', '찢', '차', '차량', '차례', '차익', '차전', '착', '찬', '찮', '찰', '참', '참여', '찹', '찼', '창', '창업', '찾', '채', '채권', '채널', '책', '책을', '책임', '챈', '챌', '챔', '챔피언', '챙', '처', '처럼', '처리', '처분', '처음', '척', '천', '천만', '천만원', '천명', '천억원', '철', '첨', '첩', '첫', '청', '청담동', '청사', '청소년', '청은', '체', '체계', '체육', '체제', '체험', '첸', '첼', '쳐', '쳤', '쳤다', '초', '초등학교', '촉', '촌', '촘', '촛', '총', '총리', '총회', '촨', '촬', '촬영', '최', '최고', '최근', '추', '추진', '축', '축구', '축구연맹', '축제', '춘', '출', '춤', '춥', '춧', '충', '춰', '췄', '췌', '취', '츄', '츠', '측', '측은', '츰', '층', '치', '치고', '치는', '치료', '치를', '칙', '친', '칠', '침', '칩', '칫', '칭', '카', '카드', '카페', '칵', '칸', '칼', '캄', '캉', '캐', '캐피탈', '캔', '캘', '캠', '캠퍼스', '캠프', '캡', '캣', '커', '컥', '컨', '컨설팅', '컫', '컬', '컴', '컴퍼니', '컵', '컷', '컸', '케', '케미칼', '케이', '켄', '켈', '켐', '켓', '켜', '켠', '켰', '코', '코드', '코리아', '코스닥', '코스피', '콕', '콘', '콘서트', '콘텐츠', '콜', '콤', '콥', '콧', '콩', '콰', '쾌', '쿄', '쿠', '쿠키', '쿡', '쿤', '쿨', '쿵', '쿼', '쿼터', '퀄', '퀘', '퀴', '퀵', '퀸', '큐', '큘', '크', '크레', '큰', '클', '클라우드', '클럽', '클리', '큼', '키', '키로', '킥', '킨', '킬', '킴', '킷', '킹', '타', '타를', '타수', '타운', '타워', '타임', '타자', '타점', '탁', '탄', '탈', '탈삼진', '탐', '탑', '탓', '탔', '탕', '태', '태영', '태평양', '택', '탠', '탤', '탬', '탭', '탰', '탱', '탱크', '터', '터미널', '턱', '턴', '털', '텀', '텁', '텃', '텅', '테', '테크', '텍', '텐', '텔', '텔레콤', '템', '텝', '텨', '톈', '토', '토록', '토크', '톡', '톤', '톨', '톰', '톱', '통', '통신', '통합', '통화', '퇴', '투', '투데이', '투수', '투어', '투자', '투자증권', '투표', '툭', '툰', '툴', '툼', '퉁', '퉈', '튀', '튕', '튜', '튠', '튬', '트', '트랙', '트로', '트리', '특', '특별', '특위', '특징주', '특히', '튼', '튿', '틀', '틈', '틋', '티', '티브', '틱', '틴', '틸', '팀', '팀장', '팁', '팅', '파', '파운드', '파이어', '파크', '팍', '팎', '판', '판매', '팔', '팜', '팝', '팟', '팠', '팡', '팥', '패', '패밀리', '패션', '팩', '팬', '팬들', '팰', '팸', '팽', '퍼', '펀', '펀드', '펄', '펌', '펑', '페', '페스티벌', '페이스', '펙', '펜', '펠', '펫', '펴', '편', '펼', '폄', '폈', '평', '평가', '평균', '폐', '포', '포럼', '포스트', '포인트', '포토', '포트', '폭', '폭력', '폰', '폴', '폴리', '폼', '퐁', '표', '표를', '푸', '푸드', '푹', '푼', '풀', '품', '풋', '풍', '퓨', '퓰', '프', '프라', '프로', '프로골프', '프로그램', '프로야구', '프리', '픈', '플', '플라이', '플랜트', '플러스', '플레이', '픔', '피', '피스', '피아', '피안타', '피해', '픽', '핀', '필', '필드', '필름', '핌', '핍', '핏', '핑', '핑크', '하', '하거나', '하게', '하겠다', '하겠다고', '하겠다는', '하고', '하기', '하기도', '하기로', '하나', '하느냐', '하는', '하는데', '하늘', '하니', '하다', '하다고', '하다는', '하더라도', '하던', '하도록', '하라', '하라고', '하려', '하려고', '하려는', '하려면', '하며', '하면', '하면서', '하면서도', '하세요', '하여', '하우스', '하이닉스', '하자', '하지', '하지만', '학', '학과', '학교', '학년', '학생', '학습', '학원', '한', '한국', '한국시간', '한다', '한다고', '한다는', '한다면', '한테', '할', '함', '함께', '함에', '함으로써', '함을', '합', '합니다', '핫', '핫이슈', '항', '항공', '해', '해달라', '해서', '해수욕장', '해야', '해온', '해왔다', '해졌다', '해주고', '해주는', '해진', '핵', '핸', '핸드', '햄', '햇', '했', '했고', '했기', '했는데', '했는지', '했다', '했다고', '했다는', '했던', '했습니다', '했어요', '했었다', '했으나', '했으며', '했으면', '했을', '했지만', '행', '행복', '행사', '행위', '행정', '향', '허', '허가', '헉', '헌', '헐', '험', '헛', '헝', '헤', '헨', '헬', '헷', '혀', '혁', '혁명', '혁신', '현', '현대', '현장', '현재', '현지시각', '현지시간', '혈', '혐', '협', '협동조합', '협력', '협상', '협의체', '협의회', '협정', '협회', '혔', '혔다', '형', '혜', '혜진', '혜택', '호', '호는', '호선', '호텔', '호텔에서', '혹', '혼', '홀', '홀딩스', '홀에서', '홈', '홈런', '홈쇼핑', '홉', '홍', '홍보', '화', '화를', '화학', '확', '확인', '환', '환경', '활', '활동', '활동을', '황', '황금어장', '회', '회담', '회를', '회말', '회사', '회의를', '회의에서', '회장', '회초', '획', '횟', '횡', '효', '효과', '효율', '후', '후보', '훈', '훈련', '훌', '훔', '훗', '훙', '훤', '훨', '훼', '휘', '휠', '휩', '휴', '흉', '흐', '흑', '흔', '흘', '흙', '흠', '흡', '흥', '흩', '희', '희망', '흰', '히', '힌', '힐', '힐링캠프', '힘', '힙']"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_score_np = word_score.cpu().numpy()"
      ],
      "metadata": {
        "id": "TNmDdxeG8xNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(word_score_np)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "UmVq41o79Lt9",
        "outputId": "7640ebbc-fcc8-4951-f8ff-e049b2c6f903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1\n",
              "0     0.000371 -0.030243\n",
              "1     0.007252  0.079079\n",
              "2    -0.004254  0.012008\n",
              "3     0.022834 -0.002270\n",
              "4    -0.058089  0.004298\n",
              "...        ...       ...\n",
              "7997  0.026606  0.015584\n",
              "7998 -0.012013 -0.009740\n",
              "7999  0.016859  0.014154\n",
              "8000  0.022297 -0.004414\n",
              "8001  0.011940 -0.035241\n",
              "\n",
              "[8002 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8d4e5684-51a1-4444-b05d-91f82aaf48c1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000371</td>\n",
              "      <td>-0.030243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.007252</td>\n",
              "      <td>0.079079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.004254</td>\n",
              "      <td>0.012008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.022834</td>\n",
              "      <td>-0.002270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.058089</td>\n",
              "      <td>0.004298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7997</th>\n",
              "      <td>0.026606</td>\n",
              "      <td>0.015584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7998</th>\n",
              "      <td>-0.012013</td>\n",
              "      <td>-0.009740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7999</th>\n",
              "      <td>0.016859</td>\n",
              "      <td>0.014154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8000</th>\n",
              "      <td>0.022297</td>\n",
              "      <td>-0.004414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8001</th>\n",
              "      <td>0.011940</td>\n",
              "      <td>-0.035241</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8002 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d4e5684-51a1-4444-b05d-91f82aaf48c1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8d4e5684-51a1-4444-b05d-91f82aaf48c1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8d4e5684-51a1-4444-b05d-91f82aaf48c1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(word_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "5a_ghYvo9St9",
        "outputId": "105561f8-970e-4f17-fcc5-32f300eb3159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0\n",
              "0      [UNK]\n",
              "1      [PAD]\n",
              "2      [CLS]\n",
              "3      [SEP]\n",
              "4     [MASK]\n",
              "...      ...\n",
              "7997       힌\n",
              "7998       힐\n",
              "7999    힐링캠프\n",
              "8000       힘\n",
              "8001       힙\n",
              "\n",
              "[8002 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-deeb7595-cf1a-48c1-ba93-3abb6aa8e36e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[UNK]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[PAD]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[CLS]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[SEP]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[MASK]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7997</th>\n",
              "      <td>힌</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7998</th>\n",
              "      <td>힐</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7999</th>\n",
              "      <td>힐링캠프</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8000</th>\n",
              "      <td>힘</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8001</th>\n",
              "      <td>힙</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8002 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-deeb7595-cf1a-48c1-ba93-3abb6aa8e36e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-deeb7595-cf1a-48c1-ba93-3abb6aa8e36e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-deeb7595-cf1a-48c1-ba93-3abb6aa8e36e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_insight = pd.concat([pd.DataFrame(word_list),pd.DataFrame(word_score_np)], axis=1)"
      ],
      "metadata": {
        "id": "tYZ94Bqw-hB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_insight.columns = ['단어', '부정','긍정']"
      ],
      "metadata": {
        "id": "TlC1MHSG-9Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 아래와 같이 각 단어와 부정, 긍정 점수를 얻을 수 있다."
      ],
      "metadata": {
        "id": "3hRibaZMXx__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_insight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "1lHtpP3VYB5n",
        "outputId": "272466ba-03bd-4b20-cb7c-45a502649e51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          단어        부정        긍정\n",
              "0      [UNK]  0.000371 -0.030243\n",
              "1      [PAD]  0.007252  0.079079\n",
              "2      [CLS] -0.004254  0.012008\n",
              "3      [SEP]  0.022834 -0.002270\n",
              "4     [MASK] -0.058089  0.004298\n",
              "...      ...       ...       ...\n",
              "7997       힌  0.026606  0.015584\n",
              "7998       힐 -0.012013 -0.009740\n",
              "7999    힐링캠프  0.016859  0.014154\n",
              "8000       힘  0.022297 -0.004414\n",
              "8001       힙  0.011940 -0.035241\n",
              "\n",
              "[8002 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cf139a6e-ce3c-4357-8315-5f2b3a5b8e82\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>단어</th>\n",
              "      <th>부정</th>\n",
              "      <th>긍정</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[UNK]</td>\n",
              "      <td>0.000371</td>\n",
              "      <td>-0.030243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[PAD]</td>\n",
              "      <td>0.007252</td>\n",
              "      <td>0.079079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[CLS]</td>\n",
              "      <td>-0.004254</td>\n",
              "      <td>0.012008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[SEP]</td>\n",
              "      <td>0.022834</td>\n",
              "      <td>-0.002270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[MASK]</td>\n",
              "      <td>-0.058089</td>\n",
              "      <td>0.004298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7997</th>\n",
              "      <td>힌</td>\n",
              "      <td>0.026606</td>\n",
              "      <td>0.015584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7998</th>\n",
              "      <td>힐</td>\n",
              "      <td>-0.012013</td>\n",
              "      <td>-0.009740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7999</th>\n",
              "      <td>힐링캠프</td>\n",
              "      <td>0.016859</td>\n",
              "      <td>0.014154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8000</th>\n",
              "      <td>힘</td>\n",
              "      <td>0.022297</td>\n",
              "      <td>-0.004414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8001</th>\n",
              "      <td>힙</td>\n",
              "      <td>0.011940</td>\n",
              "      <td>-0.035241</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8002 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf139a6e-ce3c-4357-8315-5f2b3a5b8e82')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cf139a6e-ce3c-4357-8315-5f2b3a5b8e82 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cf139a6e-ce3c-4357-8315-5f2b3a5b8e82');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_insight.sort_values('부정')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "riQo9Qz1-qrN",
        "outputId": "4049174e-05e1-4210-cd18-1eaa8fe43cc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         단어        부정        긍정\n",
              "7442      첼 -0.118954 -0.030333\n",
              "7677      틸 -0.110800 -0.028070\n",
              "5948    디스크 -0.096814 -0.001517\n",
              "7591      탈 -0.095603  0.016188\n",
              "3751    ▁이하 -0.093674  0.028851\n",
              "...     ...       ...       ...\n",
              "6449      빅  0.107193  0.036485\n",
              "5489     교사  0.107742 -0.029648\n",
              "414       j  0.108126 -0.097337\n",
              "7653      퉈  0.111610 -0.031215\n",
              "81    00000  0.114546  0.024967\n",
              "\n",
              "[8002 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b4f9c32b-9664-4e35-847e-fca58776993d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>단어</th>\n",
              "      <th>부정</th>\n",
              "      <th>긍정</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7442</th>\n",
              "      <td>첼</td>\n",
              "      <td>-0.118954</td>\n",
              "      <td>-0.030333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7677</th>\n",
              "      <td>틸</td>\n",
              "      <td>-0.110800</td>\n",
              "      <td>-0.028070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5948</th>\n",
              "      <td>디스크</td>\n",
              "      <td>-0.096814</td>\n",
              "      <td>-0.001517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7591</th>\n",
              "      <td>탈</td>\n",
              "      <td>-0.095603</td>\n",
              "      <td>0.016188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3751</th>\n",
              "      <td>▁이하</td>\n",
              "      <td>-0.093674</td>\n",
              "      <td>0.028851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6449</th>\n",
              "      <td>빅</td>\n",
              "      <td>0.107193</td>\n",
              "      <td>0.036485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5489</th>\n",
              "      <td>교사</td>\n",
              "      <td>0.107742</td>\n",
              "      <td>-0.029648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414</th>\n",
              "      <td>j</td>\n",
              "      <td>0.108126</td>\n",
              "      <td>-0.097337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7653</th>\n",
              "      <td>퉈</td>\n",
              "      <td>0.111610</td>\n",
              "      <td>-0.031215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>00000</td>\n",
              "      <td>0.114546</td>\n",
              "      <td>0.024967</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8002 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b4f9c32b-9664-4e35-847e-fca58776993d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b4f9c32b-9664-4e35-847e-fca58776993d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b4f9c32b-9664-4e35-847e-fca58776993d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 중 일부분을 살펴보면 다음과 같다. 다만, 이 점수를 통해서 명확히 무언가를 파악하기에는 어려워 보인다."
      ],
      "metadata": {
        "id": "UXTu18dga2we"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_insight[(sentiment_insight['단어'] == '▁사랑을')|\n",
        "                  (sentiment_insight['단어'] == '▁없')|\n",
        "                  (sentiment_insight['단어'] == '▁에너지')|\n",
        "                  (sentiment_insight['단어'] == '▁이벤트')|\n",
        "                  (sentiment_insight['단어'] == '▁이상의')|\n",
        "                  (sentiment_insight['단어'] == '▁많아')|\n",
        "                  (sentiment_insight['단어'] == '▁명령')|\n",
        "                  (sentiment_insight['단어'] == '▁나쁜')|\n",
        "                  (sentiment_insight['단어'] == '▁할인')\n",
        "                  ]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "Z3lkskR6ZKgP",
        "outputId": "9cdcca8a-db6e-48dd-a4b7-afef2bc3ef41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        단어        부정        긍정\n",
              "1380   ▁나쁜  0.008308 -0.049664\n",
              "1952   ▁많아 -0.043824  0.004251\n",
              "2036   ▁명령  0.034890 -0.003139\n",
              "2591  ▁사랑을 -0.008165  0.022978\n",
              "3270    ▁없  0.000439  0.000156\n",
              "3285  ▁에너지  0.040282  0.043092\n",
              "3699  ▁이벤트 -0.005541  0.003471\n",
              "3705  ▁이상의 -0.005852  0.032736\n",
              "4981   ▁할인 -0.006949 -0.031965"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c4f14121-536b-40af-adb9-f7e29733d6ed\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>단어</th>\n",
              "      <th>부정</th>\n",
              "      <th>긍정</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1380</th>\n",
              "      <td>▁나쁜</td>\n",
              "      <td>0.008308</td>\n",
              "      <td>-0.049664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1952</th>\n",
              "      <td>▁많아</td>\n",
              "      <td>-0.043824</td>\n",
              "      <td>0.004251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2036</th>\n",
              "      <td>▁명령</td>\n",
              "      <td>0.034890</td>\n",
              "      <td>-0.003139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2591</th>\n",
              "      <td>▁사랑을</td>\n",
              "      <td>-0.008165</td>\n",
              "      <td>0.022978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3270</th>\n",
              "      <td>▁없</td>\n",
              "      <td>0.000439</td>\n",
              "      <td>0.000156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3285</th>\n",
              "      <td>▁에너지</td>\n",
              "      <td>0.040282</td>\n",
              "      <td>0.043092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3699</th>\n",
              "      <td>▁이벤트</td>\n",
              "      <td>-0.005541</td>\n",
              "      <td>0.003471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3705</th>\n",
              "      <td>▁이상의</td>\n",
              "      <td>-0.005852</td>\n",
              "      <td>0.032736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4981</th>\n",
              "      <td>▁할인</td>\n",
              "      <td>-0.006949</td>\n",
              "      <td>-0.031965</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c4f14121-536b-40af-adb9-f7e29733d6ed')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c4f14121-536b-40af-adb9-f7e29733d6ed button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c4f14121-536b-40af-adb9-f7e29733d6ed');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# csv로 저장\n",
        "sentiment_insight.to_csv('11번가_감정점수.csv', encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "y9fiyBxL-3oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 감정사전을 통한 긍부정 분류"
      ],
      "metadata": {
        "id": "pdqmyFjhV26L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "gjtIywrqV7SK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/이창대_kaggle/11번가_리뷰(43만).csv', encoding='utf-8-sig', index_col=0)"
      ],
      "metadata": {
        "id": "BZxUtr4SWhHL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/park1200656/KnuSentiLex\n",
        "knu = pd.read_csv('/content/drive/MyDrive/이창대_kaggle/SentiWord_Dict.csv', encoding='utf-8',header=None)"
      ],
      "metadata": {
        "id": "M2bx4QYZWCYb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knu.columns = ['word','score']"
      ],
      "metadata": {
        "id": "plGuHBOIWKsd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "R_kiQPaXWOKZ",
        "outputId": "a163a1f3-f169-4eeb-df18-2b4464ba4a8a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        word  score\n",
              "0        (-;      1\n",
              "1      (;_;)     -1\n",
              "2       (^^)      1\n",
              "3      (^-^)      1\n",
              "4       (^^*      1\n",
              "...      ...    ...\n",
              "14850     갈등     -1\n",
              "14851     의혹     -1\n",
              "14852  내팽개치다     -2\n",
              "14853     횡령     -2\n",
              "14854    불안증     -2\n",
              "\n",
              "[14855 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0e12cacd-90dd-408d-ac75-5c1aed6da82e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(-;</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(;_;)</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(^^)</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(^-^)</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(^^*</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14850</th>\n",
              "      <td>갈등</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14851</th>\n",
              "      <td>의혹</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14852</th>\n",
              "      <td>내팽개치다</td>\n",
              "      <td>-2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14853</th>\n",
              "      <td>횡령</td>\n",
              "      <td>-2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14854</th>\n",
              "      <td>불안증</td>\n",
              "      <td>-2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14855 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e12cacd-90dd-408d-ac75-5c1aed6da82e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0e12cacd-90dd-408d-ac75-5c1aed6da82e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0e12cacd-90dd-408d-ac75-5c1aed6da82e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 감정 점수 계산하는 코드\n",
        "# 점수와 그 점수에 영향미친 단어로 계산함\n",
        "# 데이터프레임으로 가져와서 numpy 형태라 np.str 사용했고 그래서 워닝 뜸\n",
        "\n",
        "def sentiment_score(input_str):\n",
        "    score = 0\n",
        "    senti_word = []\n",
        "    for i in range(len(knu)):\n",
        "        if knu['word'][i] in input_str:\n",
        "            score += knu['score'][i]\n",
        "            senti_word.append(knu['word'][i] + ' ' + np.str(knu['score'][i]))\n",
        "        else:\n",
        "            continue\n",
        "    return score, senti_word"
      ],
      "metadata": {
        "id": "DoJ2tW1pWcDq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_score(df['리뷰'][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YNFtFzkWqcy",
        "outputId": "a173daeb-e824-4a54-c06c-eac973cb811c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-8185aef2ece9>:11: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  senti_word.append(knu['word'][i] + ' ' + np.str(knu['score'][i]))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-7, ['못 0', '믿고 1', '비싼 -2', '정 1', '탈 -1', '탈이 -1', '해 -2', '허접 -2', 'ㅜ -1'])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_score = []\n",
        "review_score_exp = []\n",
        "\n",
        "i = 0\n",
        "for review in df['리뷰']:\n",
        "    score, senti_word = sentiment_score(review)\n",
        "    review_score.append(score)\n",
        "    review_score_exp.append(senti_word)\n",
        "    \n",
        "    i+=1\n",
        "    \n",
        "    if i % 500 == 0:\n",
        "        print(i,\"번째 진행중\")\n",
        "    \n",
        "    # 2000개에 대해서만 샘플로 진행해봄\n",
        "    if i == 2000:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWZgV30WWqga",
        "outputId": "bafdbb30-984f-42e3-9a8b-9e77582cce53"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-8185aef2ece9>:11: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  senti_word.append(knu['word'][i] + ' ' + np.str(knu['score'][i]))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500 번째 진행중\n",
            "1000 번째 진행중\n",
            "1500 번째 진행중\n",
            "2000 번째 진행중\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(review_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4CXHgiNWwRL",
        "outputId": "624a1f7d-f31c-43ca-aedd-2bfce71c4cb1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sentiment_dict = pd.DataFrame([review_score, review_score_exp, df['리뷰'][:2000], df['평점'][:2000]]).transpose()"
      ],
      "metadata": {
        "id": "ujQaE65YWyH2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sentiment_dict.columns = ('score','explanation','review', 'star')"
      ],
      "metadata": {
        "id": "m8E6lv8-WyV7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sentiment_dict.sample(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "I9F3eIMbWy1y",
        "outputId": "f111afe7-37f7-494c-b10d-691c806b9845"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     score                             explanation  \\\n",
              "1781    -1                                 [ㅡㅡ -1]   \n",
              "1636    -1                                  [ㅠ -1]   \n",
              "1666     0                             [정 1, ㅜ -1]   \n",
              "1851    -2  [; 1, 성 0, 성이 0, 져서 -1, 찢어져 -1, ㅡㅡ -1]   \n",
              "457      4                    [마음대로 0, 좋아 2, 친절 2]   \n",
              "553     -5  [못 0, 비싼 -2, 없어서 -1, 함께 1, 해 -2, ㅠ -1]   \n",
              "1117    -2                          [깝 -1, 아깝다 -1]   \n",
              "244     -1                     [실망 -2, 좋아 2, ㅠ -1]   \n",
              "829     -6        [부족 -1, 부족한 -2, 상하게 -2, 이상하게 -1]   \n",
              "772     -2                          [우는 -1, 져서 -1]   \n",
              "\n",
              "                                                 review star  \n",
              "1781                                     질이.안조아여.ㅡㅡㅡ별루임    1  \n",
              "1636                     세탁하고 나니 형체가 없어졌어요ㅠㅠ빨지말고 입어야할듯~    1  \n",
              "1666  헐...오트밀 샀는데 다 비쳐요... 세상에 주머니도 비칠정도니 말 다했네요. 배송...    1  \n",
              "1851  바지 신축성이 전혀 없어요~오늘 입구나갔는데 가르쟁이(??)가 찢어져서 담요를 두르...    1  \n",
              "457   물건은 아주마음에들고 좋아요 다만 판매자보다 11번가상담원들이 불친절하고응대에미흡하...    1  \n",
              "553   비싼돈주고 20~25마리 2kg이나 주문해서 기다렸더니~~ 배송건이 많아서 이틀이나...    1  \n",
              "1117             이렇게 얇은 비닐이 있을수있나요물건을담을수나있을지돈아깝다는 생각듭니다    1  \n",
              "244   저렴하게 구입했다고 좋아했는데 재고로 만원에 판매했었던건지 만원택이 붙어있네요ㅠ 거...    1  \n",
              "829                수산대전 쿠폰때문인가..이상하게 양이 20퍼 부족한 느낌이네요..    1  \n",
              "772   매년 여기서 새우를 구매하지만, 이번 새우는 껍질이 흐물거리는 새우가 반이상 이어서...    1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ccf23311-9bd8-4129-b888-2233239c63fd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>score</th>\n",
              "      <th>explanation</th>\n",
              "      <th>review</th>\n",
              "      <th>star</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1781</th>\n",
              "      <td>-1</td>\n",
              "      <td>[ㅡㅡ -1]</td>\n",
              "      <td>질이.안조아여.ㅡㅡㅡ별루임</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1636</th>\n",
              "      <td>-1</td>\n",
              "      <td>[ㅠ -1]</td>\n",
              "      <td>세탁하고 나니 형체가 없어졌어요ㅠㅠ빨지말고 입어야할듯~</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1666</th>\n",
              "      <td>0</td>\n",
              "      <td>[정 1, ㅜ -1]</td>\n",
              "      <td>헐...오트밀 샀는데 다 비쳐요... 세상에 주머니도 비칠정도니 말 다했네요. 배송...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1851</th>\n",
              "      <td>-2</td>\n",
              "      <td>[; 1, 성 0, 성이 0, 져서 -1, 찢어져 -1, ㅡㅡ -1]</td>\n",
              "      <td>바지 신축성이 전혀 없어요~오늘 입구나갔는데 가르쟁이(??)가 찢어져서 담요를 두르...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>457</th>\n",
              "      <td>4</td>\n",
              "      <td>[마음대로 0, 좋아 2, 친절 2]</td>\n",
              "      <td>물건은 아주마음에들고 좋아요 다만 판매자보다 11번가상담원들이 불친절하고응대에미흡하...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>553</th>\n",
              "      <td>-5</td>\n",
              "      <td>[못 0, 비싼 -2, 없어서 -1, 함께 1, 해 -2, ㅠ -1]</td>\n",
              "      <td>비싼돈주고 20~25마리 2kg이나 주문해서 기다렸더니~~ 배송건이 많아서 이틀이나...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1117</th>\n",
              "      <td>-2</td>\n",
              "      <td>[깝 -1, 아깝다 -1]</td>\n",
              "      <td>이렇게 얇은 비닐이 있을수있나요물건을담을수나있을지돈아깝다는 생각듭니다</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244</th>\n",
              "      <td>-1</td>\n",
              "      <td>[실망 -2, 좋아 2, ㅠ -1]</td>\n",
              "      <td>저렴하게 구입했다고 좋아했는데 재고로 만원에 판매했었던건지 만원택이 붙어있네요ㅠ 거...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>829</th>\n",
              "      <td>-6</td>\n",
              "      <td>[부족 -1, 부족한 -2, 상하게 -2, 이상하게 -1]</td>\n",
              "      <td>수산대전 쿠폰때문인가..이상하게 양이 20퍼 부족한 느낌이네요..</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>772</th>\n",
              "      <td>-2</td>\n",
              "      <td>[우는 -1, 져서 -1]</td>\n",
              "      <td>매년 여기서 새우를 구매하지만, 이번 새우는 껍질이 흐물거리는 새우가 반이상 이어서...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ccf23311-9bd8-4129-b888-2233239c63fd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ccf23311-9bd8-4129-b888-2233239c63fd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ccf23311-9bd8-4129-b888-2233239c63fd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sentiment_dict.to_csv('df_sentiment.csv',encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "ty5LtutjWy6B"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}